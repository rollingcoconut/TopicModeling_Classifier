{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Divison1: Topic Modeling work and cosine similarity search on articles featured as topics\n",
    "\n",
    "### PRINCIPLES:\n",
    "* Separtion of code from data\n",
    "* Minimally Viable Product\n",
    "\n",
    "### ORGANIZATION:\n",
    "* ./DATA\n",
    "* ./CORPUSES\n",
    "* ./MODELS\n",
    "* requirements.txt\n",
    "* create_corpus.py\n",
    "* create_models.py\n",
    "\n",
    "### SYSTEM DEVELOPMENT ENVIRONMENT ADVICE\n",
    "* Best to install a new virtual environment in python \n",
    "* The code was tested in ipython terminal\n",
    "* All code is developed in python3 for python3 systems\n",
    "\n",
    "ex.: Install code dependencies after creating new virtualenvenn\n",
    "\n",
    " (potential way to set up environment)\n",
    "    > mkvirtualenv --python=/usr/bin/python3 summerpy3\n",
    "    > workon summerpy3\n",
    "    ~/.virtualenvs/summerpy3/bin/pip3 install ipython[notebook]\n",
    " (get python 3 requirements)\n",
    "    >  ~/.virtualenvs/summerpy3/bin/pip install -r requirements.txt \n",
    " (start following along with tutorial)\n",
    "    > ipython\n",
    "    > %run ...\n",
    "    ...\n",
    "    > deactivate\n",
    "\n",
    "### ATTRIBUTIONS\n",
    "The data-->corpus-->model work I've done is based/inspired on gensim creator's online tutorials:\n",
    "    http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html\n",
    "  \n",
    "The word cloud work is based/inspired on code from \"Building Machine Learning Systems with Python by Richert and Coelho, available under MIT license\n",
    "\n",
    "\n",
    "### TODO \n",
    "1. Biggest thing to work on is an update capacity; currently to update the LDA model, you need to create a new corpus and new model\n",
    "    \n",
    "2. Merge code written for Collocation and NER cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING  LEMMATIZED CORPUS FROM DIRECTORY OF .TXT FILES\n",
    "1. **Create corpuses if you have a directory of .txt files.** \n",
    "\n",
    "    Save corpuses (.mm) and\n",
    "    dictionaries (.dict) for  lemmatized processed data\n",
    "\n",
    "    The lemmitized corpus will be saved in ./CORPUSES (Script uses relative path for CORPUSES location.)\n",
    "    \n",
    "        ./create_corpuses.py  dir_txt_files corpus_name --nodup(flag to add duplicate content only 1x) \n",
    "        \n",
    "    >   **./create_corpuses.py \"./DATA/content\"  \"db\"**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "** Let's create a corpus for **./DATA/content** which has 186 database articles **\n",
    "\n",
    "Three files \n",
    "       * corpus_name.dict \n",
    "       * corpus_name.mm\n",
    "       * corpus_name.mm_index*\n",
    "will be placed in ./CORPUSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : built Dictionary(14414 unique tokens: ['richard', 'clinical', 'portalrecent', 'escrow', 'iyaz']...) from 186 documents (total 119911 corpus positions)\n",
      "INFO : saving Dictionary object under ./CORPUSES/_db_lemmatized.dict, separately None\n",
      "INFO : storing corpus in Matrix Market format to ./CORPUSES/_db_lemmatized.mm\n",
      "INFO : saving sparse matrix to ./CORPUSES/_db_lemmatized.mm\n",
      "INFO : PROGRESS: saving document #0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style is lemmatized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : saved 186x14414 matrix, density=2.460% (65955/2681004)\n",
      "DEBUG : closing ./CORPUSES/_db_lemmatized.mm\n",
      "DEBUG : closing ./CORPUSES/_db_lemmatized.mm\n",
      "INFO : saving MmCorpus index to ./CORPUSES/_db_lemmatized.mm.index\n"
     ]
    }
   ],
   "source": [
    "%run create_corpuses.py \"DATA/content\" _db  # --nodup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## GOING FROM CORPUS TO MODEL\n",
    "1. **Create models from corpuses in ./CORPUSES. **\n",
    "\n",
    "        create_models.py corpus_name num_topics no_of_passes\n",
    "\n",
    "    * **corpus_name**: use the same name you gave in the step to create corpuses form the .txt file\n",
    "    * **no_topics**: how many topics should LDA model contain\n",
    "    * **no_of_passes**: how many iterations to tra\n",
    "    in model\n",
    ">   **./create_models.py  \"_db\"   100   40**\n",
    "\n",
    "\n",
    "\n",
    "The models will be saved in ./MODELS. \n",
    "\n",
    "**NOTE**: cosine similarity function takes a function from \"create_corpus.py\"\n",
    "\n",
    "### Loading corpuses and models\n",
    "> dictionary = corpora.Dictionary.load(\"./CORPUSES/_db_lemmatized.dict\")\n",
    "\n",
    "> model = gensim.models.LdaModel.load(\"./MODELS/_db_tfidf_lda_model.model\")\n",
    "\n",
    "> corpus = gensim.corpora.MmCorpus(\"./CORPUSES/_db_lemmatized.mm)\n",
    "\n",
    "-----\n",
    "**Get LDA model for the privacy database (~ 186 articles)**\n",
    "\n",
    "With 100 topics and 40 passes, this will take a few minutes.\n",
    "\n",
    "I've already saved the model files that will get produced in ./MODELS**. Let's just load them **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loading LdaModel object from ./MODELS/_db_lemmatized_tfidf_lda.model\n",
      "INFO : loading id2word recursively from ./MODELS/_db_lemmatized_tfidf_lda.model.id2word.* with mmap=None\n",
      "INFO : setting ignored attribute dispatcher to None\n",
      "INFO : setting ignored attribute state to None\n",
      "INFO : loading LdaModel object from ./MODELS/_db_lemmatized_tfidf_lda.model.state\n",
      "INFO : loading Dictionary object from ./CORPUSES/_db_lemmatized.dict\n",
      "INFO : loaded corpus index from ./CORPUSES/_db_lemmatized.mm.index\n",
      "INFO : initializing corpus reader from ./CORPUSES/_db_lemmatized.mm\n",
      "INFO : accepted corpus with 186 documents, 14414 features, 65955 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "# I've already saved models for the corpus _db, so I will load it\n",
    "import gensim\n",
    "privacy_lda_tfidf_model  = gensim.models.LdaModel.load(\"./MODELS/_db_lemmatized_tfidf_lda.model\")\n",
    "privacy_lemmatized_dict= gensim.corpora.Dictionary.load(\"./CORPUSES/_db_lemmatized.dict\")\n",
    "privacy_lemmatized_corpus = gensim.corpora.MmCorpus(\"./CORPUSES/_db_lemmatized.mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### SOME COOL THINGS TO DO WITH MODELS: cosine_similarity btw two articles in DB\n",
    "** Let's use our model to find the cosine similarity between two articles in the DB **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_sim score for g1 and g2 should be high, and it is 0.947885\n",
      "cos_sim score for g1 and g3 should be medium, and it is 0.716024\n",
      "cos_sim score for g2 and g3 should be medium, and it is 0.691978\n"
     ]
    }
   ],
   "source": [
    "from create_models import *\n",
    "\n",
    "# some articles from the DB\n",
    "art1fn = \"google search autocomplete feature links a german .txt\"\n",
    "art2fn = \"google search autocomplete links german prime mini.txt\"\n",
    "art3fn = \"some germans dislike having photos of their homes .txt\"\n",
    "\n",
    "# Clearly art1 and art2 are more related to each other than to art3\n",
    "# Let's see if our  cosine similarity gets this right as well\n",
    "\n",
    "STORY_CONTENT_DIR = \"./DATA/content\"\n",
    "\n",
    "print(\"cos_sim score for g1 and g2 should be high, and it is %f\" %topic_relev_score(art1fn,art2fn, STORY_CONTENT_DIR, privacy_lemmatized_dict, privacy_lda_tfidf_model))\n",
    "print(\"cos_sim score for g1 and g3 should be medium, and it is %f\" %topic_relev_score(art1fn,art3fn, STORY_CONTENT_DIR, privacy_lemmatized_dict, privacy_lda_tfidf_model))\n",
    "print(\"cos_sim score for g2 and g3 should be medium, and it is %f\" %topic_relev_score(art2fn,art3fn, STORY_CONTENT_DIR, privacy_lemmatized_dict, privacy_lda_tfidf_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOME COOL THINGS TO DO WITH MODELS: Top 10 topics\n",
    "** Get 10 top topics of a corpus, with n words/phrases of each topic **\n",
    "\n",
    "The function **create_models.panda_topics(path_to_model, path_to_corpus)** just needs the location  of the Model you want to examine, and the location of the  corresponding corpus that made the model\n",
    "\n",
    "**NOTE** To use this function you need  to have at least created the model with at least 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loading LdaModel object from ./MODELS/_db_lemmatized_tfidf_lda.model\n",
      "INFO : loading id2word recursively from ./MODELS/_db_lemmatized_tfidf_lda.model.id2word.* with mmap=None\n",
      "INFO : setting ignored attribute dispatcher to None\n",
      "INFO : setting ignored attribute state to None\n",
      "INFO : loading LdaModel object from ./MODELS/_db_lemmatized_tfidf_lda.model.state\n",
      "INFO : loaded corpus index from ./CORPUSES/_db_lemmatized.mm.index\n",
      "INFO : initializing corpus reader from ./CORPUSES/_db_lemmatized.mm\n",
      "INFO : accepted corpus with 186 documents, 14414 features, 65955 non-zero entries\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1st Most Discussed Topic (MDT)</th>\n",
       "      <td>0.008*data + 0.006*breach + 0.006*said + 0.006*customer + 0.005*record + 0.004*school + 0.004*information + 0.004*patient + 0.004*online + 0.004*twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2nd MDT</th>\n",
       "      <td>0.013*user + 0.012*facebook + 0.008*security + 0.007*site + 0.006*network + 0.006*photo + 0.006*setting + 0.006*friend + 0.005*feature + 0.005*policy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd MDT</th>\n",
       "      <td>0.011*apps + 0.008*child + 0.007*snapchat + 0.007*website + 0.005*video + 0.005*number + 0.005*government + 0.005*personal + 0.005*party + 0.004*digital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4th MDT</th>\n",
       "      <td>0.010*credit + 0.009*location + 0.008*affected + 0.005*http + 0.005*week + 0.005*statement + 0.004*open + 0.004*researcher + 0.004*file + 0.004*iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5th MDT</th>\n",
       "      <td>0.015*google + 0.012*app + 0.009*student + 0.008*address + 0.007*court + 0.006*york + 0.006*mail + 0.006*law + 0.006*developer + 0.006*post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6th MDT</th>\n",
       "      <td>0.010*search + 0.007*nytimes + 0.006*com + 0.006*rule + 0.005*european + 0.005*archive + 0.005*www + 0.005*engine + 0.005*term + 0.004*campaign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7th MDT</th>\n",
       "      <td>0.008*consumer + 0.007*web + 0.007*phone + 0.007*say + 0.006*cooky + 0.006*target + 0.006*message + 0.006*state + 0.005*technology + 0.005*collected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8th MDT</th>\n",
       "      <td>0.012*sony + 0.010*card + 0.008*compromised + 0.007*stolen + 0.007*million + 0.007*theft + 0.005*payment + 0.004*breach + 0.004*tjx + 0.004*exposed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9th MDT</th>\n",
       "      <td>0.007*agency + 0.007*city + 0.006*agent + 0.005*worker + 0.005*database + 0.004*general + 0.004*victim + 0.004*mother + 0.004*family + 0.004*attorney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10th MDT</th>\n",
       "      <td>0.007*supra + 0.004*pregnant + 0.004*published + 0.004*teen + 0.003*subject + 0.003*didfalse + 0.003*duke + 0.003*father + 0.003*info + 0.003*secure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                       0\n",
       "1st Most Discussed Topic (MDT)  0.008*data + 0.006*breach + 0.006*said + 0.006*customer + 0.005*record + 0.004*school + 0.004*information + 0.004*patient + 0.004*online + 0.004*twitter\n",
       "2nd MDT                            0.013*user + 0.012*facebook + 0.008*security + 0.007*site + 0.006*network + 0.006*photo + 0.006*setting + 0.006*friend + 0.005*feature + 0.005*policy\n",
       "3rd MDT                         0.011*apps + 0.008*child + 0.007*snapchat + 0.007*website + 0.005*video + 0.005*number + 0.005*government + 0.005*personal + 0.005*party + 0.004*digital\n",
       "4th MDT                           0.010*credit + 0.009*location + 0.008*affected + 0.005*http + 0.005*week + 0.005*statement + 0.004*open + 0.004*researcher + 0.004*file + 0.004*iphone\n",
       "5th MDT                                      0.015*google + 0.012*app + 0.009*student + 0.008*address + 0.007*court + 0.006*york + 0.006*mail + 0.006*law + 0.006*developer + 0.006*post\n",
       "6th MDT                                  0.010*search + 0.007*nytimes + 0.006*com + 0.006*rule + 0.005*european + 0.005*archive + 0.005*www + 0.005*engine + 0.005*term + 0.004*campaign\n",
       "7th MDT                             0.008*consumer + 0.007*web + 0.007*phone + 0.007*say + 0.006*cooky + 0.006*target + 0.006*message + 0.006*state + 0.005*technology + 0.005*collected\n",
       "8th MDT                              0.012*sony + 0.010*card + 0.008*compromised + 0.007*stolen + 0.007*million + 0.007*theft + 0.005*payment + 0.004*breach + 0.004*tjx + 0.004*exposed\n",
       "9th MDT                            0.007*agency + 0.007*city + 0.006*agent + 0.005*worker + 0.005*database + 0.004*general + 0.004*victim + 0.004*mother + 0.004*family + 0.004*attorney\n",
       "10th MDT                            0.007*supra + 0.004*pregnant + 0.004*published + 0.004*teen + 0.003*subject + 0.003*didfalse + 0.003*duke + 0.003*father + 0.003*info + 0.003*secure"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import importlib\n",
    "#importlib.reload(create_models)\n",
    "from create_models import *\n",
    "\n",
    "panda_topics(\"./MODELS/_db_lemmatized_tfidf_lda.model\" , \"./CORPUSES/_db_lemmatized.mm\", 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### SOME COOL THINGS TO DO WITH MODELS: Brangelina top topics post-divorce announcement\n",
    "\n",
    "** I'm interested to know what the top topics are with the Branjelina breakup. **\n",
    "\n",
    "I did a quick bing scrape for Angelina Jolie, placing 54 bing articles in **./DATA/jolie**. \n",
    "\n",
    "Let's make a corpus and model and then run our **panda_topics()** function to see what the top topics are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : built Dictionary(3085 unique tokens: ['richard', 'phnom', 'clinical', 'pitt', 'parenthood']...) from 54 documents (total 10317 corpus positions)\n",
      "INFO : saving Dictionary object under ./CORPUSES/jolie_lemmatized.dict, separately None\n",
      "INFO : storing corpus in Matrix Market format to ./CORPUSES/jolie_lemmatized.mm\n",
      "INFO : saving sparse matrix to ./CORPUSES/jolie_lemmatized.mm\n",
      "INFO : ITER_DB_ARTILCES():duplicate files below not added twice to content_dump of script:  \n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : saved 54x3085 matrix, density=3.974% (6621/166590)\n",
      "DEBUG : closing ./CORPUSES/jolie_lemmatized.mm\n",
      "DEBUG : closing ./CORPUSES/jolie_lemmatized.mm\n",
      "INFO : saving MmCorpus index to ./CORPUSES/jolie_lemmatized.mm.index\n",
      "INFO : loaded corpus index from ./CORPUSES/jolie_lemmatized.mm.index\n",
      "INFO : initializing corpus reader from ./CORPUSES/jolie_lemmatized.mm\n",
      "INFO : accepted corpus with 54 documents, 3085 features, 6621 non-zero entries\n",
      "INFO : loading Dictionary object from ./CORPUSES/jolie_lemmatized.dict\n",
      "INFO : collecting document frequencies\n",
      "INFO : PROGRESS: processing document #0\n",
      "INFO : calculating IDF weights for 54 documents and 3084 features (6621 matrix non-zeros)\n",
      "INFO : using symmetric alpha at 0.02\n",
      "INFO : using symmetric eta at 0.02\n",
      "INFO : using serial LDA version on this node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style is lemmatized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : running online LDA training, 50 topics, 40 passes over the supplied corpus of 54 documents, updating model once every 54 documents, evaluating perplexity every 54 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -852.239 per-word bound, 35439916659840407091196100302870322717915435831171396527926175548848177660243666142773353116512915303453414827474387434327858924501419221902095291231966614659277778881055296232306937233013282980062351104922699030208200889210820573636634554411913778361794560.0 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 0, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #14 (0.020): 0.001*couple + 0.001*close + 0.001*divorce + 0.001*feliz + 0.001*jennifer + 0.001*source + 0.001*los + 0.001*pitt + 0.001*miraval + 0.001*cotillard\n",
      "INFO : topic #25 (0.020): 0.003*rape + 0.002*inevitable + 0.002*sexual + 0.002*rapist + 0.002*survivor + 0.002*shame + 0.001*violence + 0.001*global + 0.001*urged + 0.001*disgrace\n",
      "INFO : topic #41 (0.020): 0.006*grain + 0.004*food + 0.004*ancient + 0.003*affleck + 0.003*angie + 0.003*meat + 0.003*eating + 0.003*seed + 0.003*diet + 0.003*bird\n",
      "INFO : topic #15 (0.020): 0.006*peacekeeping + 0.003*protection + 0.003*woman + 0.003*mom + 0.003*heart + 0.002*nation + 0.002*right + 0.002*shed + 0.002*promise + 0.002*syria\n",
      "INFO : topic #3 (0.020): 0.004*quarter + 0.004*com + 0.004*french + 0.003*curbed + 0.003*market + 0.002*home + 0.002*house + 0.002*housed + 0.002*recording + 0.002*masonry\n",
      "INFO : topic diff=44.531509, rho=1.000000\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -45.420 per-word bound, 47079293034735.5 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 1, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #11 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #44 (0.020): 0.004*showcase + 0.004*till + 0.004*voting + 0.004*brien + 0.004*grown + 0.004*running + 0.004*clip + 0.004*political + 0.004*conan + 0.004*wait\n",
      "INFO : topic #22 (0.020): 0.005*scene + 0.005*movie + 0.004*normal + 0.004*father + 0.004*killed + 0.004*oldest + 0.003*pitt + 0.003*onscreen + 0.003*son + 0.003*red\n",
      "INFO : topic #35 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #47 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic diff=0.218484, rho=0.577350\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -44.165 per-word bound, 19725793774114.1 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 2, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #21 (0.020): 0.006*heard + 0.004*statement + 0.004*press + 0.004*news + 0.004*list + 0.003*privacy + 0.003*later + 0.003*ask + 0.003*challenging + 0.003*space\n",
      "INFO : topic #15 (0.020): 0.006*peacekeeping + 0.004*protection + 0.003*heart + 0.003*woman + 0.003*mom + 0.003*republic + 0.003*african + 0.003*involvement + 0.003*defence + 0.003*central\n",
      "INFO : topic #37 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #49 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #20 (0.020): 0.007*marvel + 0.006*gazing + 0.006*horoscope + 0.005*finally + 0.005*captain + 0.004*future + 0.004*finished + 0.004*focus + 0.004*stop + 0.004*far\n",
      "INFO : topic diff=0.152765, rho=0.500000\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -43.522 per-word bound, 12629976027906.6 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 3, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #26 (0.020): 0.008*source + 0.007*report + 0.006*cotillard + 0.006*marion + 0.006*pitt + 0.006*divorce + 0.006*like + 0.005*destructive + 0.005*want + 0.005*brad\n",
      "INFO : topic #5 (0.020): 0.004*ailing + 0.003*caused + 0.003*relationship + 0.003*health + 0.003*london + 0.003*moving + 0.002*fight + 0.002*claimed + 0.002*remained + 0.002*entire\n",
      "INFO : topic #42 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #27 (0.020): 0.008*child + 0.008*divorce + 0.007*couple + 0.007*split + 0.007*year + 0.007*old + 0.006*filed + 0.006*family + 0.006*brad + 0.006*actor\n",
      "INFO : topic #13 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic diff=0.102693, rho=0.447214\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -43.179 per-word bound, 9960144879020.1 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 4, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #33 (0.020): 0.004*refugee + 0.004*country + 0.004*troubled + 0.004*later + 0.004*war + 0.003*high + 0.003*crisis + 0.003*activist + 0.003*zone + 0.003*commissioner\n",
      "INFO : topic #7 (0.020): 0.005*hoax + 0.005*suicide + 0.004*fan + 0.003*disturbing + 0.003*dead + 0.003*august + 0.002*died + 0.002*comment + 0.002*totally + 0.002*sad\n",
      "INFO : topic #36 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #18 (0.020): 0.006*bloom + 0.005*maleficent + 0.005*say + 0.004*evil + 0.004*curse + 0.003*nup + 0.003*analyst + 0.003*lisa + 0.003*surrounding + 0.003*stated\n",
      "INFO : topic #25 (0.020): 0.002*urged + 0.002*disgrace + 0.002*blame + 0.001*seeing + 0.001*touch + 0.001*crime + 0.001*sexual + 0.001*rape + 0.000*inevitable + 0.000*rapist\n",
      "INFO : topic diff=0.070989, rho=0.408248\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.976 per-word bound, 8650546751618.5 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 5, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #22 (0.020): 0.006*father + 0.006*killed + 0.005*scene + 0.005*son + 0.004*normal + 0.004*cambodia + 0.004*movie + 0.004*red + 0.004*carpet + 0.004*directed\n",
      "INFO : topic #24 (0.020): 0.011*inbox + 0.007*latest + 0.006*loung + 0.006*ung + 0.005*day + 0.005*giveaway + 0.005*content + 0.005*signup + 0.005*news + 0.005*trailer\n",
      "INFO : topic #26 (0.020): 0.009*source + 0.007*report + 0.007*pitt + 0.006*cotillard + 0.006*marion + 0.006*like + 0.005*want + 0.005*divorce + 0.005*destructive + 0.005*brad\n",
      "INFO : topic #15 (0.020): 0.006*peacekeeping + 0.004*protection + 0.003*heart + 0.003*woman + 0.003*mom + 0.003*african + 0.003*republic + 0.003*defence + 0.003*central + 0.003*involvement\n",
      "INFO : topic #21 (0.020): 0.007*heard + 0.005*news + 0.005*statement + 0.004*press + 0.004*kid + 0.004*later + 0.004*list + 0.004*matter + 0.004*privacy + 0.004*ask\n",
      "INFO : topic diff=0.050473, rho=0.377964\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.829 per-word bound, 7813189498975.5 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 6, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #28 (0.020): 0.005*read + 0.004*exposed + 0.004*sandwich + 0.004*starving + 0.004*eat + 0.003*surprising + 0.002*love + 0.002*kim + 0.002*shocking + 0.002*course\n",
      "INFO : topic #43 (0.020): 0.006*aniston + 0.003*jennifer + 0.003*los + 0.003*custody + 0.003*smith + 0.003*mr + 0.002*filing + 0.002*johnny + 0.002*bad + 0.002*allegation\n",
      "INFO : topic #10 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #4 (0.020): 0.008*woman + 0.006*film + 0.005*movie + 0.004*certainly + 0.004*career + 0.004*acting + 0.004*director + 0.004*star + 0.004*debut + 0.003*drama\n",
      "INFO : topic #29 (0.020): 0.007*divorce + 0.004*bohringer + 0.004*radar + 0.004*marriage + 0.003*embed + 0.003*skip + 0.003*loading + 0.003*drinking + 0.003*funny + 0.003*radaronline\n",
      "INFO : topic diff=0.036491, rho=0.353553\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.732 per-word bound, 7302904751602.2 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 7, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #49 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #10 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #16 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #31 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #2 (0.020): 0.000*starving + 0.000*eat + 0.000*sandwich + 0.000*basement + 0.000*nourishment + 0.000*conveniently + 0.000*solved + 0.000*boom + 0.000*save + 0.000*repeat\n",
      "INFO : topic diff=0.027105, rho=0.333333\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.659 per-word bound, 6943731847914.8 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 8, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #16 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #2 (0.020): 0.000*starving + 0.000*eat + 0.000*sandwich + 0.000*basement + 0.000*nourishment + 0.000*conveniently + 0.000*solved + 0.000*boom + 0.000*save + 0.000*repeat\n",
      "INFO : topic #24 (0.020): 0.011*inbox + 0.007*latest + 0.007*loung + 0.006*ung + 0.005*content + 0.005*giveaway + 0.005*signup + 0.005*news + 0.005*trailer + 0.005*khmer\n",
      "INFO : topic #21 (0.020): 0.007*heard + 0.006*news + 0.005*statement + 0.005*kid + 0.005*press + 0.005*tuesday + 0.004*later + 0.004*matter + 0.004*list + 0.004*privacy\n",
      "INFO : topic #20 (0.020): 0.007*marvel + 0.006*gazing + 0.006*horoscope + 0.005*finally + 0.005*captain + 0.004*future + 0.004*finished + 0.004*stop + 0.004*far + 0.004*superhero\n",
      "INFO : topic diff=0.020427, rho=0.316228\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.606 per-word bound, 6693225212890.1 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 9, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #35 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #18 (0.020): 0.006*bloom + 0.006*say + 0.005*maleficent + 0.004*evil + 0.004*curse + 0.003*nup + 0.003*lisa + 0.003*surrounding + 0.003*stated + 0.003*analyst\n",
      "INFO : topic #46 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #47 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #28 (0.020): 0.005*read + 0.004*exposed + 0.004*sandwich + 0.004*starving + 0.004*eat + 0.003*love + 0.003*surprising + 0.002*shocking + 0.002*course + 0.002*kim\n",
      "INFO : topic diff=0.015535, rho=0.301511\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.568 per-word bound, 6517977868956.7 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 10, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #28 (0.020): 0.005*read + 0.004*exposed + 0.004*sandwich + 0.004*starving + 0.004*eat + 0.003*love + 0.003*surprising + 0.002*shocking + 0.002*course + 0.002*kim\n",
      "INFO : topic #34 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #22 (0.020): 0.007*father + 0.007*killed + 0.006*cambodia + 0.005*son + 0.005*scene + 0.005*movie + 0.005*people + 0.004*normal + 0.004*directed + 0.004*red\n",
      "INFO : topic #30 (0.020): 0.005*netflix + 0.004*universal + 0.004*project + 0.004*sea + 0.003*unbroken + 0.003*production + 0.003*produce + 0.003*revving + 0.003*chairman + 0.003*donna\n",
      "INFO : topic #25 (0.020): 0.002*urged + 0.002*disgrace + 0.002*blame + 0.002*seeing + 0.001*touch + 0.000*crime + 0.000*sexual + 0.000*rape + 0.000*inevitable + 0.000*rapist\n",
      "INFO : topic diff=0.011934, rho=0.288675\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.543 per-word bound, 6407393052529.9 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 11, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #1 (0.020): 0.007*frankenstein + 0.005*clinic + 0.004*bride + 0.004*preventative + 0.004*procedure + 0.003*mummy + 0.003*referral + 0.003*number + 0.003*prevention + 0.003*january\n",
      "INFO : topic #43 (0.020): 0.006*aniston + 0.004*jennifer + 0.003*los + 0.003*smith + 0.003*mr + 0.003*filing + 0.002*custody + 0.002*engaged + 0.002*johnny + 0.002*bad\n",
      "INFO : topic #4 (0.020): 0.009*film + 0.009*woman + 0.005*acting + 0.005*movie + 0.004*certainly + 0.004*career + 0.004*star + 0.004*oscar + 0.004*director + 0.004*working\n",
      "INFO : topic #5 (0.020): 0.005*ailing + 0.003*relationship + 0.003*caused + 0.003*london + 0.003*moving + 0.003*health + 0.002*fight + 0.002*remained + 0.002*entire + 0.002*admitted\n",
      "INFO : topic #15 (0.020): 0.007*peacekeeping + 0.005*protection + 0.003*heart + 0.003*woman + 0.003*involvement + 0.003*republic + 0.003*central + 0.003*defence + 0.003*african + 0.003*mom\n",
      "INFO : topic diff=0.009432, rho=0.277350\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.521 per-word bound, 6311492751966.0 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 12, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #25 (0.020): 0.002*urged + 0.002*disgrace + 0.002*blame + 0.002*seeing + 0.001*touch + 0.000*crime + 0.000*sexual + 0.000*rape + 0.000*inevitable + 0.000*rapist\n",
      "INFO : topic #33 (0.020): 0.005*country + 0.005*refugee + 0.004*later + 0.004*crisis + 0.004*troubled + 0.004*war + 0.004*including + 0.003*high + 0.003*human + 0.003*activist\n",
      "INFO : topic #21 (0.020): 0.007*heard + 0.006*news + 0.005*kid + 0.005*statement + 0.005*tuesday + 0.005*press + 0.004*later + 0.004*matter + 0.004*list + 0.004*privacy\n",
      "INFO : topic #0 (0.020): 0.006*teigen + 0.006*mara + 0.004*pdt + 0.004*marateigen_ + 0.004*posted + 0.003*smiling + 0.002*kyliecosmetics + 0.002*mar + 0.002*apr + 0.002*resemblance\n",
      "INFO : topic #26 (0.020): 0.009*source + 0.007*report + 0.006*cotillard + 0.006*marion + 0.006*like + 0.006*want + 0.005*destructive + 0.005*set + 0.005*claim + 0.005*divorce\n",
      "INFO : topic diff=0.007631, rho=0.267261\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.503 per-word bound, 6231202265346.6 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 13, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #39 (0.020): 0.004*peacekeeping + 0.004*summit + 0.003*piero + 0.003*lansbury + 0.003*steve + 0.003*honoree + 0.003*governor + 0.003*angela + 0.003*martin + 0.003*indelible\n",
      "INFO : topic #27 (0.020): 0.011*divorce + 0.011*child + 0.010*pitt + 0.010*brad + 0.010*couple + 0.008*split + 0.008*family + 0.008*year + 0.007*said + 0.007*filed\n",
      "INFO : topic #44 (0.020): 0.004*brien + 0.004*political + 0.004*showcase + 0.004*grown + 0.004*conan + 0.004*clip + 0.004*running + 0.004*till + 0.004*voting + 0.004*wait\n",
      "INFO : topic #5 (0.020): 0.005*ailing + 0.003*relationship + 0.003*caused + 0.003*london + 0.003*moving + 0.003*health + 0.002*fight + 0.002*remained + 0.002*entire + 0.002*admitted\n",
      "INFO : topic #33 (0.020): 0.005*country + 0.005*refugee + 0.004*later + 0.004*crisis + 0.004*troubled + 0.004*war + 0.004*including + 0.003*high + 0.003*human + 0.003*activist\n",
      "INFO : topic diff=0.006316, rho=0.258199\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.484 per-word bound, 6152741324865.6 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 14, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #14 (0.020): 0.002*feliz + 0.002*miraval + 0.002*chateau + 0.002*main + 0.002*featured + 0.002*guillaume + 0.002*compound + 0.001*ceremony + 0.001*canet + 0.001*rented\n",
      "INFO : topic #13 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #25 (0.020): 0.002*urged + 0.002*disgrace + 0.002*blame + 0.002*seeing + 0.001*touch + 0.000*crime + 0.000*sexual + 0.000*rape + 0.000*inevitable + 0.000*rapist\n",
      "INFO : topic #18 (0.020): 0.006*bloom + 0.006*say + 0.005*maleficent + 0.004*evil + 0.004*curse + 0.003*nup + 0.003*stated + 0.003*lisa + 0.003*surrounding + 0.003*analyst\n",
      "INFO : topic #20 (0.020): 0.007*marvel + 0.006*gazing + 0.006*horoscope + 0.005*finally + 0.005*captain + 0.004*future + 0.004*finished + 0.004*far + 0.004*stop + 0.004*superhero\n",
      "INFO : topic diff=0.005324, rho=0.250000\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.466 per-word bound, 6076321494642.5 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 15, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #11 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #46 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #12 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #13 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #26 (0.020): 0.009*source + 0.007*report + 0.006*cotillard + 0.006*marion + 0.006*like + 0.006*want + 0.005*destructive + 0.005*claim + 0.005*set + 0.005*photo\n",
      "INFO : topic diff=0.004478, rho=0.242536\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.452 per-word bound, 6016652662236.9 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 16, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #37 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #42 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #15 (0.020): 0.007*peacekeeping + 0.005*protection + 0.003*heart + 0.003*woman + 0.003*involvement + 0.003*republic + 0.003*defence + 0.003*african + 0.003*central + 0.003*mom\n",
      "INFO : topic #8 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #40 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic diff=0.003822, rho=0.235702\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.442 per-word bound, 5973292520704.4 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 17, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #24 (0.020): 0.011*inbox + 0.007*latest + 0.007*loung + 0.006*ung + 0.005*content + 0.005*giveaway + 0.005*signup + 0.005*news + 0.005*trailer + 0.005*khmer\n",
      "INFO : topic #48 (0.020): 0.003*brangelina + 0.003*diaz + 0.002*biggest + 0.002*era + 0.002*soap + 0.002*glamour + 0.001*segued + 0.001*exposure + 0.001*shouldn + 0.001*kanye\n",
      "INFO : topic #26 (0.020): 0.009*source + 0.007*report + 0.006*cotillard + 0.006*marion + 0.006*like + 0.006*want + 0.005*destructive + 0.005*claim + 0.005*set + 0.005*photo\n",
      "INFO : topic #20 (0.020): 0.007*marvel + 0.006*gazing + 0.006*horoscope + 0.005*finally + 0.005*captain + 0.004*future + 0.004*finished + 0.004*far + 0.004*stop + 0.004*superhero\n",
      "INFO : topic #19 (0.020): 0.004*say + 0.004*talked + 0.004*pose + 0.003*understand + 0.003*question + 0.003*enjoy + 0.003*fun + 0.003*read + 0.003*nice + 0.003*week\n",
      "INFO : topic diff=0.003304, rho=0.229416\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.431 per-word bound, 5929928161685.4 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 18, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #33 (0.020): 0.006*country + 0.005*refugee + 0.004*later + 0.004*crisis + 0.004*troubled + 0.004*war + 0.004*including + 0.003*high + 0.003*human + 0.003*activist\n",
      "INFO : topic #12 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #47 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #18 (0.020): 0.006*bloom + 0.006*say + 0.005*maleficent + 0.004*evil + 0.004*curse + 0.003*nup + 0.003*analyst + 0.003*surrounding + 0.003*stated + 0.003*lisa\n",
      "INFO : topic #1 (0.020): 0.007*frankenstein + 0.006*clinic + 0.004*bride + 0.004*preventative + 0.004*procedure + 0.003*mummy + 0.003*referral + 0.003*number + 0.003*prevention + 0.003*january\n",
      "INFO : topic diff=0.002968, rho=0.223607\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.420 per-word bound, 5884256400808.4 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 19, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #28 (0.020): 0.005*read + 0.004*exposed + 0.004*sandwich + 0.004*starving + 0.004*eat + 0.003*love + 0.003*surprising + 0.003*shocking + 0.002*course + 0.002*sexy\n",
      "INFO : topic #46 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #20 (0.020): 0.007*marvel + 0.006*horoscope + 0.006*gazing + 0.005*finally + 0.005*captain + 0.004*future + 0.004*finished + 0.004*far + 0.004*stop + 0.004*superhero\n",
      "INFO : topic #2 (0.020): 0.000*starving + 0.000*eat + 0.000*sandwich + 0.000*basement + 0.000*nourishment + 0.000*conveniently + 0.000*solved + 0.000*boom + 0.000*save + 0.000*repeat\n",
      "INFO : topic #41 (0.020): 0.006*grain + 0.005*affleck + 0.004*ben + 0.004*cop + 0.004*food + 0.004*ancient + 0.004*angie + 0.003*truth + 0.003*gossip + 0.003*diet\n",
      "INFO : topic diff=0.002671, rho=0.218218\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.411 per-word bound, 5849171599390.3 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 20, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #0 (0.020): 0.006*teigen + 0.006*mara + 0.004*pdt + 0.004*marateigen_ + 0.004*posted + 0.003*smiling + 0.002*kyliecosmetics + 0.002*mar + 0.002*apr + 0.002*resemblance\n",
      "INFO : topic #19 (0.020): 0.004*say + 0.004*talked + 0.004*pose + 0.003*understand + 0.003*question + 0.003*enjoy + 0.003*fun + 0.003*nice + 0.003*read + 0.003*week\n",
      "INFO : topic #4 (0.020): 0.012*film + 0.009*woman + 0.005*acting + 0.005*oscar + 0.005*movie + 0.005*star + 0.005*hollywood + 0.004*director + 0.004*certainly + 0.004*think\n",
      "INFO : topic #49 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #20 (0.020): 0.007*marvel + 0.006*gazing + 0.006*horoscope + 0.005*finally + 0.005*captain + 0.004*future + 0.004*finished + 0.004*stop + 0.004*far + 0.004*superhero\n",
      "INFO : topic diff=0.002379, rho=0.213201\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.403 per-word bound, 5815741806360.9 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 21, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #10 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #1 (0.020): 0.007*frankenstein + 0.006*clinic + 0.004*bride + 0.004*preventative + 0.004*procedure + 0.003*mummy + 0.003*referral + 0.003*number + 0.003*prevention + 0.003*january\n",
      "INFO : topic #47 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #23 (0.020): 0.008*george + 0.002*divorcing + 0.002*brangelina + 0.002*saying + 0.002*stunned + 0.002*suffocation + 0.002*cnnent + 0.002*fgczp + 0.002*fame + 0.002*ho\n",
      "INFO : topic #36 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic diff=0.002105, rho=0.208514\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.397 per-word bound, 5791149113735.9 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 22, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #8 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #12 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #20 (0.020): 0.007*marvel + 0.006*gazing + 0.006*horoscope + 0.005*finally + 0.005*captain + 0.004*future + 0.004*finished + 0.004*stop + 0.004*far + 0.004*superhero\n",
      "INFO : topic #41 (0.020): 0.006*grain + 0.005*affleck + 0.004*ben + 0.004*cop + 0.004*food + 0.004*ancient + 0.004*angie + 0.003*truth + 0.003*gossip + 0.003*eating\n",
      "INFO : topic #39 (0.020): 0.004*peacekeeping + 0.004*summit + 0.003*piero + 0.003*lansbury + 0.003*tosi + 0.003*governor + 0.003*steve + 0.003*angela + 0.003*honoree + 0.003*indelible\n",
      "INFO : topic diff=0.001870, rho=0.204124\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.388 per-word bound, 5756654221445.4 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 23, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #11 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #47 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #4 (0.020): 0.012*film + 0.009*woman + 0.005*star + 0.005*oscar + 0.005*acting + 0.005*movie + 0.005*hollywood + 0.004*director + 0.004*certainly + 0.004*think\n",
      "INFO : topic #32 (0.020): 0.005*politics + 0.004*zamperini + 0.003*conscious + 0.003*sent + 0.002*december + 0.002*war + 0.002*regularly + 0.002*physically + 0.002*body + 0.002*noted\n",
      "INFO : topic #36 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic diff=0.001699, rho=0.200000\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.382 per-word bound, 5731420420806.6 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 24, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #4 (0.020): 0.012*film + 0.009*woman + 0.005*star + 0.005*oscar + 0.005*acting + 0.005*movie + 0.005*hollywood + 0.004*director + 0.004*certainly + 0.004*think\n",
      "INFO : topic #48 (0.020): 0.003*brangelina + 0.003*diaz + 0.002*biggest + 0.002*era + 0.002*soap + 0.002*glamour + 0.001*segued + 0.001*exposure + 0.001*shouldn + 0.001*kanye\n",
      "INFO : topic #2 (0.020): 0.000*starving + 0.000*eat + 0.000*sandwich + 0.000*basement + 0.000*nourishment + 0.000*conveniently + 0.000*solved + 0.000*boom + 0.000*save + 0.000*repeat\n",
      "INFO : topic #9 (0.020): 0.006*close + 0.005*death + 0.004*grave + 0.004*tomorrow + 0.004*investigator + 0.004*hired + 0.004*private + 0.003*angie + 0.003*fly + 0.003*rush\n",
      "INFO : topic #27 (0.020): 0.014*divorce + 0.013*pitt + 0.012*brad + 0.010*child + 0.010*couple + 0.009*said + 0.008*split + 0.008*family + 0.008*year + 0.007*filed\n",
      "INFO : topic diff=0.001548, rho=0.196116\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.378 per-word bound, 5715357353244.7 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 25, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #18 (0.020): 0.006*bloom + 0.005*maleficent + 0.004*evil + 0.004*say + 0.004*curse + 0.003*nup + 0.003*lisa + 0.003*neglect + 0.003*stated + 0.003*surrounding\n",
      "INFO : topic #36 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #0 (0.020): 0.006*teigen + 0.006*mara + 0.004*pdt + 0.004*marateigen_ + 0.004*posted + 0.003*smiling + 0.002*kyliecosmetics + 0.002*mar + 0.002*apr + 0.002*resemblance\n",
      "INFO : topic #25 (0.020): 0.002*urged + 0.002*disgrace + 0.002*blame + 0.002*seeing + 0.001*touch + 0.000*crime + 0.000*sexual + 0.000*rape + 0.000*inevitable + 0.000*rapist\n",
      "INFO : topic #30 (0.020): 0.006*universal + 0.005*netflix + 0.004*sea + 0.004*project + 0.003*production + 0.003*unbroken + 0.003*produce + 0.003*script + 0.003*revving + 0.003*chairman\n",
      "INFO : topic diff=0.001461, rho=0.192450\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.374 per-word bound, 5699279178682.2 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 26, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #16 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #38 (0.020): 0.008*try + 0.008*locate + 0.008*browse + 0.006*requested + 0.005*sorry + 0.005*page + 0.000*couple + 0.000*source + 0.000*los + 0.000*divorce\n",
      "INFO : topic #33 (0.020): 0.006*country + 0.005*refugee + 0.004*later + 0.004*crisis + 0.004*troubled + 0.004*war + 0.004*including + 0.003*high + 0.003*human + 0.003*activist\n",
      "INFO : topic #19 (0.020): 0.004*talked + 0.004*pose + 0.004*say + 0.003*understand + 0.003*question + 0.003*enjoy + 0.003*fun + 0.003*nice + 0.003*week + 0.002*certain\n",
      "INFO : topic #12 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic diff=0.001379, rho=0.188982\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.368 per-word bound, 5675487810138.4 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 27, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #35 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #19 (0.020): 0.004*talked + 0.004*pose + 0.004*say + 0.003*understand + 0.003*question + 0.003*enjoy + 0.003*fun + 0.003*nice + 0.003*week + 0.002*certain\n",
      "INFO : topic #30 (0.020): 0.006*universal + 0.005*netflix + 0.004*sea + 0.004*project + 0.003*production + 0.003*unbroken + 0.003*produce + 0.003*script + 0.003*revving + 0.003*chairman\n",
      "INFO : topic #7 (0.020): 0.005*suicide + 0.005*hoax + 0.004*fan + 0.003*disturbing + 0.003*dead + 0.002*died + 0.002*frenzy + 0.002*false + 0.002*reacting + 0.002*hollywoodlife\n",
      "INFO : topic #3 (0.020): 0.005*french + 0.004*quarter + 0.004*com + 0.004*market + 0.003*curbed + 0.002*home + 0.002*foundation + 0.002*million + 0.002*path + 0.002*orleans\n",
      "INFO : topic diff=0.001283, rho=0.185695\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.362 per-word bound, 5653053333150.5 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 28, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #35 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #49 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #6 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #29 (0.020): 0.005*bohringer + 0.005*radar + 0.004*marriage + 0.004*loading + 0.004*skip + 0.004*embed + 0.003*drinking + 0.003*file + 0.003*radaronline + 0.003*funny\n",
      "INFO : topic #26 (0.020): 0.009*source + 0.007*report + 0.006*cotillard + 0.006*marion + 0.006*like + 0.006*want + 0.005*say + 0.005*claim + 0.005*destructive + 0.005*set\n",
      "INFO : topic diff=0.001179, rho=0.182574\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.357 per-word bound, 5632141535988.6 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 29, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #35 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #49 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #41 (0.020): 0.006*grain + 0.005*affleck + 0.004*ben + 0.004*cop + 0.004*food + 0.004*ancient + 0.004*angie + 0.003*truth + 0.003*gossip + 0.003*diet\n",
      "INFO : topic #24 (0.020): 0.011*inbox + 0.007*latest + 0.007*loung + 0.006*ung + 0.005*signup + 0.005*content + 0.005*giveaway + 0.005*news + 0.005*trailer + 0.005*khmer\n",
      "INFO : topic #25 (0.020): 0.002*urged + 0.002*disgrace + 0.002*blame + 0.002*seeing + 0.001*touch + 0.000*crime + 0.000*sexual + 0.000*rape + 0.000*inevitable + 0.000*rapist\n",
      "INFO : topic diff=0.001076, rho=0.179605\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.352 per-word bound, 5613978334165.7 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 30, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #41 (0.020): 0.006*grain + 0.005*affleck + 0.004*ben + 0.004*cop + 0.004*food + 0.004*ancient + 0.004*angie + 0.003*truth + 0.003*gossip + 0.003*diet\n",
      "INFO : topic #40 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #43 (0.020): 0.006*aniston + 0.004*jennifer + 0.003*engaged + 0.003*los + 0.003*smith + 0.003*mr + 0.003*filing + 0.002*johnny + 0.002*bad + 0.002*allegation\n",
      "INFO : topic #2 (0.020): 0.000*starving + 0.000*eat + 0.000*sandwich + 0.000*basement + 0.000*nourishment + 0.000*conveniently + 0.000*solved + 0.000*boom + 0.000*save + 0.000*repeat\n",
      "INFO : topic #16 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic diff=0.000980, rho=0.176777\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.351 per-word bound, 5607548962501.0 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 31, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #41 (0.020): 0.006*grain + 0.005*affleck + 0.004*ben + 0.004*cop + 0.004*food + 0.004*ancient + 0.004*angie + 0.003*truth + 0.003*gossip + 0.003*diet\n",
      "INFO : topic #9 (0.020): 0.006*close + 0.005*death + 0.004*grave + 0.004*tomorrow + 0.004*hired + 0.004*investigator + 0.004*private + 0.003*angie + 0.003*fly + 0.003*rush\n",
      "INFO : topic #20 (0.020): 0.007*marvel + 0.006*gazing + 0.006*horoscope + 0.005*finally + 0.005*captain + 0.004*future + 0.004*finished + 0.004*stop + 0.004*far + 0.004*superhero\n",
      "INFO : topic #44 (0.020): 0.004*voting + 0.004*brien + 0.004*grown + 0.004*political + 0.004*till + 0.004*running + 0.004*conan + 0.004*showcase + 0.004*clip + 0.004*wait\n",
      "INFO : topic #24 (0.020): 0.011*inbox + 0.007*latest + 0.007*loung + 0.006*ung + 0.005*signup + 0.005*giveaway + 0.005*content + 0.005*news + 0.005*trailer + 0.005*khmer\n",
      "INFO : topic diff=0.000889, rho=0.174078\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.347 per-word bound, 5593995995901.5 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 32, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #5 (0.020): 0.005*ailing + 0.003*relationship + 0.003*caused + 0.003*london + 0.003*moving + 0.002*fight + 0.002*remained + 0.002*entire + 0.002*admitted + 0.002*currently\n",
      "INFO : topic #19 (0.020): 0.004*pose + 0.004*talked + 0.003*understand + 0.003*say + 0.003*question + 0.003*enjoy + 0.003*fun + 0.003*nice + 0.003*week + 0.002*certain\n",
      "INFO : topic #7 (0.020): 0.005*hoax + 0.005*suicide + 0.004*fan + 0.003*disturbing + 0.003*dead + 0.002*died + 0.002*frenzy + 0.002*false + 0.002*hollywoodlifers + 0.002*caption\n",
      "INFO : topic #42 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #45 (0.020): 0.019*cancer + 0.015*breast + 0.008*gene + 0.007*mastectomy + 0.006*risk + 0.005*surgery + 0.005*double + 0.005*brca + 0.004*mutation + 0.004*preventive\n",
      "INFO : topic diff=0.000842, rho=0.171499\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.344 per-word bound, 5580460006792.0 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 33, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #7 (0.020): 0.005*suicide + 0.005*hoax + 0.004*fan + 0.003*disturbing + 0.003*dead + 0.002*died + 0.002*frenzy + 0.002*false + 0.002*spinelli + 0.002*lol\n",
      "INFO : topic #8 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #17 (0.020): 0.006*rape + 0.004*hand + 0.004*inevitable + 0.003*month + 0.003*march + 0.003*london + 0.003*war + 0.003*violence + 0.003*shame + 0.003*survivor\n",
      "INFO : topic #10 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #40 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic diff=0.000782, rho=0.169031\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.340 per-word bound, 5567513168749.0 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 34, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #47 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #8 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #29 (0.020): 0.005*radar + 0.005*bohringer + 0.004*marriage + 0.004*embed + 0.004*skip + 0.004*loading + 0.003*drinking + 0.003*file + 0.003*funny + 0.003*radaronline\n",
      "INFO : topic #10 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #15 (0.020): 0.007*peacekeeping + 0.005*protection + 0.003*heart + 0.003*woman + 0.003*involvement + 0.003*republic + 0.003*defence + 0.003*african + 0.003*central + 0.003*mom\n",
      "INFO : topic diff=0.000713, rho=0.166667\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.339 per-word bound, 5561638908316.6 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 35, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #14 (0.020): 0.002*feliz + 0.002*miraval + 0.002*chateau + 0.002*main + 0.002*featured + 0.002*guillaume + 0.002*compound + 0.001*ceremony + 0.001*canet + 0.001*rented\n",
      "INFO : topic #4 (0.020): 0.012*film + 0.009*woman + 0.006*star + 0.005*oscar + 0.005*hollywood + 0.005*movie + 0.005*acting + 0.004*director + 0.004*certainly + 0.004*think\n",
      "INFO : topic #34 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #18 (0.020): 0.006*bloom + 0.005*maleficent + 0.004*evil + 0.004*say + 0.004*curse + 0.003*nup + 0.003*stated + 0.003*surrounding + 0.003*neglect + 0.003*lisa\n",
      "INFO : topic #13 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic diff=0.000654, rho=0.164399\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.337 per-word bound, 5555337750507.9 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 36, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #31 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #29 (0.020): 0.005*radar + 0.005*bohringer + 0.004*marriage + 0.004*loading + 0.004*skip + 0.004*embed + 0.003*drinking + 0.003*file + 0.003*funny + 0.003*radaronline\n",
      "INFO : topic #22 (0.020): 0.007*father + 0.007*killed + 0.007*cambodia + 0.005*son + 0.005*movie + 0.005*scene + 0.005*people + 0.005*tell + 0.005*work + 0.004*normal\n",
      "INFO : topic #24 (0.020): 0.011*inbox + 0.007*latest + 0.007*loung + 0.006*ung + 0.005*giveaway + 0.005*signup + 0.005*content + 0.005*news + 0.005*trailer + 0.005*khmer\n",
      "INFO : topic #25 (0.020): 0.002*urged + 0.002*disgrace + 0.002*blame + 0.002*seeing + 0.001*touch + 0.000*crime + 0.000*sexual + 0.000*rape + 0.000*inevitable + 0.000*rapist\n",
      "INFO : topic diff=0.000616, rho=0.162221\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.335 per-word bound, 5547657760933.2 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 37, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #21 (0.020): 0.007*heard + 0.007*news + 0.006*kid + 0.005*matter + 0.005*tuesday + 0.005*statement + 0.005*press + 0.004*later + 0.004*list + 0.004*privacy\n",
      "INFO : topic #18 (0.020): 0.006*bloom + 0.005*maleficent + 0.004*evil + 0.004*say + 0.004*curse + 0.003*nup + 0.003*lisa + 0.003*neglect + 0.003*stated + 0.003*analyst\n",
      "INFO : topic #22 (0.020): 0.007*father + 0.007*killed + 0.007*cambodia + 0.005*son + 0.005*movie + 0.005*scene + 0.005*people + 0.005*tell + 0.005*work + 0.004*normal\n",
      "INFO : topic #39 (0.020): 0.004*peacekeeping + 0.004*summit + 0.003*honoree + 0.003*piero + 0.003*martin + 0.003*governor + 0.003*lansbury + 0.003*indelible + 0.003*angela + 0.003*tosi\n",
      "INFO : topic #36 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic diff=0.000572, rho=0.160128\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.333 per-word bound, 5539532333197.3 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 38, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #41 (0.020): 0.006*grain + 0.005*affleck + 0.004*ben + 0.004*cop + 0.004*ancient + 0.004*food + 0.004*angie + 0.003*truth + 0.003*meat + 0.003*seed\n",
      "INFO : topic #26 (0.020): 0.009*source + 0.006*report + 0.006*cotillard + 0.006*marion + 0.006*like + 0.006*say + 0.006*want + 0.005*claim + 0.005*destructive + 0.005*set\n",
      "INFO : topic #19 (0.020): 0.004*pose + 0.004*talked + 0.004*understand + 0.003*question + 0.003*enjoy + 0.003*fun + 0.003*say + 0.003*nice + 0.003*week + 0.002*certain\n",
      "INFO : topic #22 (0.020): 0.007*father + 0.007*killed + 0.007*cambodia + 0.005*son + 0.005*movie + 0.005*scene + 0.005*people + 0.005*tell + 0.005*work + 0.004*normal\n",
      "INFO : topic #49 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic diff=0.000549, rho=0.158114\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -42.331 per-word bound, 5531102570880.2 perplexity estimate based on a held-out corpus of 54 documents with 458 words\n",
      "INFO : PROGRESS: pass 39, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #45 (0.020): 0.019*cancer + 0.015*breast + 0.008*gene + 0.007*mastectomy + 0.006*risk + 0.005*surgery + 0.005*double + 0.005*brca + 0.004*mutation + 0.004*preventive\n",
      "INFO : topic #26 (0.020): 0.009*source + 0.006*report + 0.006*cotillard + 0.006*marion + 0.006*like + 0.006*say + 0.006*want + 0.005*claim + 0.005*destructive + 0.005*set\n",
      "INFO : topic #14 (0.020): 0.002*feliz + 0.002*miraval + 0.002*chateau + 0.002*main + 0.002*featured + 0.002*guillaume + 0.002*compound + 0.001*ceremony + 0.001*canet + 0.001*rented\n",
      "INFO : topic #46 (0.020): 0.000*follows + 0.000*destroyed + 0.000*newborn + 0.000*largely + 0.000*revenge + 0.000*classic + 0.000*curse + 0.000*mistress + 0.000*ruin + 0.000*princess\n",
      "INFO : topic #38 (0.020): 0.008*try + 0.008*locate + 0.008*browse + 0.006*requested + 0.005*sorry + 0.005*page + 0.000*couple + 0.000*source + 0.000*los + 0.000*divorce\n",
      "INFO : topic diff=0.000524, rho=0.156174\n",
      "INFO : using symmetric alpha at 0.02\n",
      "INFO : using symmetric eta at 0.02\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 50 topics, 40 passes over the supplied corpus of 54 documents, updating model once every 54 documents, evaluating perplexity every 54 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -46.181 per-word bound, 79764858671877.7 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 0, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 10/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #45 (0.020): 0.022*jolie + 0.020*woman + 0.014*angelina + 0.009*peacekeeping + 0.008*cancer + 0.008*pitt + 0.007*right + 0.006*breast + 0.006*actress + 0.005*know\n",
      "INFO : topic #49 (0.020): 0.028*pitt + 0.018*jolie + 0.017*couple + 0.013*divorce + 0.012*brad + 0.012*year + 0.010*angelina + 0.007*marriage + 0.006*source + 0.006*child\n",
      "INFO : topic #7 (0.020): 0.013*pitt + 0.010*jolie + 0.008*divorce + 0.007*couple + 0.006*angelina + 0.006*child + 0.005*brad + 0.005*said + 0.004*woman + 0.004*source\n",
      "INFO : topic #18 (0.020): 0.019*pitt + 0.017*jolie + 0.011*divorce + 0.008*child + 0.008*couple + 0.007*brad + 0.006*angelina + 0.005*source + 0.005*said + 0.004*year\n",
      "INFO : topic #26 (0.020): 0.038*jolie + 0.029*pitt + 0.028*angelina + 0.025*brad + 0.019*divorce + 0.011*film + 0.008*child + 0.006*report + 0.005*say + 0.005*year\n",
      "INFO : topic diff=31.955545, rho=1.000000\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -13.339 per-word bound, 10359.9 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 1, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 48/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #28 (0.020): 0.022*movie + 0.013*jolie + 0.013*day + 0.011*angelina + 0.009*star + 0.008*year + 0.007*film + 0.007*news + 0.007*story + 0.007*world\n",
      "INFO : topic #32 (0.020): 0.003*jolie + 0.003*angelina + 0.003*pitt + 0.003*brad + 0.002*divorce + 0.002*cotillard + 0.002*marion + 0.002*couple + 0.002*report + 0.001*star\n",
      "INFO : topic #47 (0.020): 0.047*pitt + 0.036*jolie + 0.014*movie + 0.014*brad + 0.012*child + 0.011*angelina + 0.010*divorce + 0.007*year + 0.007*gossip + 0.007*father\n",
      "INFO : topic #21 (0.020): 0.011*pitt + 0.008*jolie + 0.006*woman + 0.005*angelina + 0.004*brad + 0.004*year + 0.004*breast + 0.003*divorce + 0.003*cancer + 0.003*film\n",
      "INFO : topic #16 (0.020): 0.027*frankenstein + 0.015*jolie + 0.015*bride + 0.012*pitt + 0.011*mummy + 0.011*universal + 0.008*character + 0.008*time + 0.008*film + 0.008*awful\n",
      "INFO : topic diff=3.736482, rho=0.577350\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -10.770 per-word bound, 1745.7 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 2, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 53/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #6 (0.020): 0.022*jolie + 0.021*woman + 0.015*angelina + 0.011*breast + 0.010*awareness + 0.009*cancer + 0.007*like + 0.007*treatment + 0.007*raise + 0.006*said\n",
      "INFO : topic #36 (0.020): 0.003*jolie + 0.002*film + 0.002*pitt + 0.002*angelina + 0.001*like + 0.001*woman + 0.001*couple + 0.001*mara + 0.001*pdt + 0.001*teigen\n",
      "INFO : topic #7 (0.020): 0.005*pitt + 0.004*jolie + 0.003*divorce + 0.003*couple + 0.002*angelina + 0.002*child + 0.002*brad + 0.002*said + 0.002*woman + 0.001*source\n",
      "INFO : topic #31 (0.020): 0.025*pitt + 0.022*jolie + 0.021*celebrity + 0.015*divorce + 0.012*brangelina + 0.010*couple + 0.010*said + 0.008*medium + 0.008*time + 0.008*child\n",
      "INFO : topic #23 (0.020): 0.046*jolie + 0.045*angelina + 0.037*pitt + 0.035*brad + 0.015*news + 0.012*george + 0.011*report + 0.011*said + 0.009*year + 0.009*couple\n",
      "INFO : topic diff=2.795319, rho=0.500000\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -9.767 per-word bound, 871.4 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 3, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 53/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #10 (0.020): 0.001*pitt + 0.001*jolie + 0.001*brad + 0.001*source + 0.001*couple + 0.001*said + 0.001*affleck + 0.001*angelina + 0.001*cop + 0.001*child\n",
      "INFO : topic #30 (0.020): 0.018*jolie + 0.015*child + 0.015*new + 0.015*people + 0.014*currently + 0.014*latest + 0.014*coordinate + 0.013*border + 0.010*pitt + 0.009*angelina\n",
      "INFO : topic #2 (0.020): 0.039*jolie + 0.031*heard + 0.013*pitt + 0.010*say + 0.010*think + 0.010*depp + 0.009*woman + 0.007*amber + 0.007*gun + 0.007*sander\n",
      "INFO : topic #9 (0.020): 0.004*pitt + 0.003*jolie + 0.002*couple + 0.002*divorce + 0.002*said + 0.002*angelina + 0.002*child + 0.001*year + 0.001*time + 0.001*brad\n",
      "INFO : topic #5 (0.020): 0.033*maleficent + 0.023*jolie + 0.019*film + 0.013*evil + 0.011*come + 0.011*story + 0.010*curse + 0.009*way + 0.008*fanning + 0.008*king\n",
      "INFO : topic diff=2.044434, rho=0.447214\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -9.265 per-word bound, 615.1 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 4, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 53/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #41 (0.020): 0.002*woman + 0.001*pitt + 0.001*cancer + 0.001*jolie + 0.001*breast + 0.001*angelina + 0.001*brad + 0.001*think + 0.001*affleck + 0.001*tabloid\n",
      "INFO : topic #48 (0.020): 0.039*jolie + 0.026*angelina + 0.021*pitt + 0.020*brad + 0.019*new + 0.014*normal + 0.010*movie + 0.010*maleficent + 0.010*red + 0.010*carpet\n",
      "INFO : topic #15 (0.020): 0.005*pitt + 0.004*jolie + 0.002*angelina + 0.002*brad + 0.002*cancer + 0.002*divorce + 0.001*couple + 0.001*year + 0.001*risk + 0.001*child\n",
      "INFO : topic #0 (0.020): 0.065*cancer + 0.041*breast + 0.026*risk + 0.025*gene + 0.021*woman + 0.019*brca + 0.015*surgery + 0.015*jolie + 0.013*ovarian + 0.013*pitt\n",
      "INFO : topic #22 (0.020): 0.028*say + 0.024*film + 0.021*jolie + 0.014*question + 0.014*read + 0.013*enjoy + 0.012*angelina + 0.010*week + 0.010*war + 0.010*time\n",
      "INFO : topic diff=1.469968, rho=0.408248\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.998 per-word bound, 511.4 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 5, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #20 (0.020): 0.001*frankenstein + 0.001*jolie + 0.001*bride + 0.001*rape + 0.001*universal + 0.001*pitt + 0.001*producer + 0.001*mummy + 0.001*lagoon + 0.001*film\n",
      "INFO : topic #24 (0.020): 0.030*jolie + 0.027*pitt + 0.019*couple + 0.015*child + 0.014*said + 0.014*angelina + 0.013*source + 0.011*year + 0.011*brad + 0.010*time\n",
      "INFO : topic #0 (0.020): 0.065*cancer + 0.041*breast + 0.027*risk + 0.025*gene + 0.020*woman + 0.020*brca + 0.016*surgery + 0.015*jolie + 0.014*ovarian + 0.013*pitt\n",
      "INFO : topic #46 (0.020): 0.028*peacekeeping + 0.028*woman + 0.021*right + 0.014*nation + 0.014*protection + 0.014*heart + 0.014*mom + 0.014*jolie + 0.014*actress + 0.007*conflict\n",
      "INFO : topic #47 (0.020): 0.049*pitt + 0.034*jolie + 0.019*movie + 0.013*brad + 0.011*gossip + 0.011*tabloid + 0.011*cop + 0.011*affleck + 0.010*child + 0.010*angelina\n",
      "INFO : topic diff=1.048484, rho=0.377964\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.852 per-word bound, 462.1 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 6, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #36 (0.020): 0.001*jolie + 0.001*film + 0.001*pitt + 0.001*angelina + 0.000*like + 0.000*woman + 0.000*couple + 0.000*mara + 0.000*pdt + 0.000*teigen\n",
      "INFO : topic #49 (0.020): 0.049*pitt + 0.029*divorce + 0.025*jolie + 0.020*couple + 0.020*year + 0.018*brad + 0.015*marriage + 0.015*bohringer + 0.015*radar + 0.015*drinking\n",
      "INFO : topic #41 (0.020): 0.001*woman + 0.001*pitt + 0.001*cancer + 0.001*jolie + 0.001*breast + 0.001*angelina + 0.001*brad + 0.001*think + 0.000*affleck + 0.000*tabloid\n",
      "INFO : topic #15 (0.020): 0.002*pitt + 0.002*jolie + 0.001*angelina + 0.001*brad + 0.001*cancer + 0.001*divorce + 0.001*couple + 0.001*year + 0.001*risk + 0.001*child\n",
      "INFO : topic #9 (0.020): 0.001*pitt + 0.001*jolie + 0.001*couple + 0.001*divorce + 0.001*said + 0.001*angelina + 0.001*child + 0.001*year + 0.001*time + 0.001*brad\n",
      "INFO : topic diff=0.746387, rho=0.353553\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.769 per-word bound, 436.3 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 7, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #30 (0.020): 0.017*jolie + 0.016*child + 0.016*new + 0.016*people + 0.016*currently + 0.016*latest + 0.016*coordinate + 0.016*border + 0.009*pitt + 0.008*angelina\n",
      "INFO : topic #32 (0.020): 0.000*jolie + 0.000*angelina + 0.000*pitt + 0.000*brad + 0.000*divorce + 0.000*cotillard + 0.000*marion + 0.000*couple + 0.000*report + 0.000*star\n",
      "INFO : topic #22 (0.020): 0.030*say + 0.025*film + 0.021*jolie + 0.015*question + 0.015*read + 0.015*enjoy + 0.011*week + 0.011*angelina + 0.010*war + 0.010*time\n",
      "INFO : topic #42 (0.020): 0.001*jolie + 0.001*pitt + 0.001*woman + 0.001*angelina + 0.001*cancer + 0.001*double + 0.001*clinic + 0.001*surgery + 0.001*preventative + 0.001*heard\n",
      "INFO : topic #9 (0.020): 0.001*pitt + 0.001*jolie + 0.001*couple + 0.001*divorce + 0.001*said + 0.001*angelina + 0.001*child + 0.001*year + 0.001*time + 0.000*brad\n",
      "INFO : topic diff=0.532440, rho=0.333333\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.721 per-word bound, 421.9 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 8, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #15 (0.020): 0.001*pitt + 0.001*jolie + 0.001*angelina + 0.001*brad + 0.001*cancer + 0.001*divorce + 0.001*couple + 0.001*year + 0.001*risk + 0.001*child\n",
      "INFO : topic #29 (0.020): 0.023*clinic + 0.020*jolie + 0.018*procedure + 0.018*angelina + 0.017*preventative + 0.017*mastectomy + 0.017*double + 0.015*woman + 0.015*number + 0.015*prevention\n",
      "INFO : topic #16 (0.020): 0.033*frankenstein + 0.019*bride + 0.014*jolie + 0.014*mummy + 0.014*universal + 0.010*character + 0.010*pitt + 0.010*working + 0.010*film + 0.010*time\n",
      "INFO : topic #4 (0.020): 0.032*jolie + 0.028*pitt + 0.020*angelina + 0.017*brad + 0.016*year + 0.016*child + 0.012*split + 0.011*news + 0.010*destructive + 0.010*self\n",
      "INFO : topic #33 (0.020): 0.001*cancer + 0.001*woman + 0.000*surgery + 0.000*breast + 0.000*jolie + 0.000*mastectomy + 0.000*risk + 0.000*clinic + 0.000*double + 0.000*angelina\n",
      "INFO : topic diff=0.381619, rho=0.316228\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.692 per-word bound, 413.5 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 9, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #23 (0.020): 0.047*jolie + 0.047*angelina + 0.038*pitt + 0.036*brad + 0.016*news + 0.014*george + 0.011*report + 0.011*said + 0.009*year + 0.009*couple\n",
      "INFO : topic #16 (0.020): 0.033*frankenstein + 0.019*bride + 0.014*jolie + 0.014*mummy + 0.014*universal + 0.010*character + 0.010*working + 0.010*pitt + 0.010*film + 0.010*time\n",
      "INFO : topic #39 (0.020): 0.043*jolie + 0.038*film + 0.023*angelina + 0.016*marvel + 0.014*africa + 0.012*woman + 0.011*captain + 0.009*like + 0.008*wonder + 0.008*studio\n",
      "INFO : topic #12 (0.020): 0.025*detail + 0.016*project + 0.013*tell + 0.013*today + 0.013*finally + 0.013*focus + 0.013*won + 0.013*far + 0.013*fact + 0.013*future\n",
      "INFO : topic #5 (0.020): 0.036*maleficent + 0.022*jolie + 0.019*film + 0.014*evil + 0.013*come + 0.012*story + 0.011*curse + 0.010*way + 0.009*fanning + 0.009*king\n",
      "INFO : topic diff=0.275272, rho=0.301511\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.674 per-word bound, 408.4 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 10, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #23 (0.020): 0.047*jolie + 0.047*angelina + 0.038*pitt + 0.036*brad + 0.016*news + 0.014*george + 0.011*report + 0.011*said + 0.009*year + 0.009*couple\n",
      "INFO : topic #14 (0.020): 0.033*rape + 0.020*sexual + 0.020*inevitable + 0.019*war + 0.013*shame + 0.013*violence + 0.013*feel + 0.013*global + 0.013*law + 0.013*rapist\n",
      "INFO : topic #35 (0.020): 0.025*grain + 0.015*brad + 0.015*source + 0.015*food + 0.015*ancient + 0.015*angie + 0.010*angelina + 0.010*said + 0.010*like + 0.010*year\n",
      "INFO : topic #47 (0.020): 0.049*pitt + 0.034*jolie + 0.020*movie + 0.012*brad + 0.011*gossip + 0.011*tabloid + 0.011*cop + 0.011*affleck + 0.009*child + 0.009*angelina\n",
      "INFO : topic #48 (0.020): 0.040*jolie + 0.025*angelina + 0.020*pitt + 0.020*brad + 0.020*new + 0.015*normal + 0.010*movie + 0.010*maleficent + 0.010*red + 0.010*carpet\n",
      "INFO : topic diff=0.200038, rho=0.288675\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.662 per-word bound, 405.2 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 11, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #23 (0.020): 0.047*jolie + 0.047*angelina + 0.038*pitt + 0.036*brad + 0.016*news + 0.014*george + 0.011*report + 0.011*said + 0.009*year + 0.009*couple\n",
      "INFO : topic #4 (0.020): 0.032*jolie + 0.028*pitt + 0.020*angelina + 0.017*brad + 0.017*year + 0.015*child + 0.012*split + 0.011*news + 0.010*destructive + 0.010*self\n",
      "INFO : topic #13 (0.020): 0.014*movie + 0.014*want + 0.014*news + 0.014*newsletter + 0.014*content + 0.014*giveaway + 0.014*signup + 0.014*latest + 0.014*exclusive + 0.014*trailer\n",
      "INFO : topic #5 (0.020): 0.037*maleficent + 0.022*jolie + 0.019*film + 0.014*evil + 0.013*come + 0.012*story + 0.011*curse + 0.010*way + 0.009*fanning + 0.009*king\n",
      "INFO : topic #37 (0.020): 0.001*jolie + 0.000*pitt + 0.000*film + 0.000*woman + 0.000*angelina + 0.000*said + 0.000*heard + 0.000*khmer + 0.000*loung + 0.000*cancer\n",
      "INFO : topic diff=0.146542, rho=0.277350\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.655 per-word bound, 403.1 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 12, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #22 (0.020): 0.030*say + 0.026*film + 0.021*jolie + 0.015*question + 0.015*read + 0.015*enjoy + 0.011*week + 0.010*war + 0.010*angelina + 0.010*hollywood\n",
      "INFO : topic #31 (0.020): 0.029*jolie + 0.025*celebrity + 0.024*pitt + 0.014*brangelina + 0.013*couple + 0.013*said + 0.012*divorce + 0.011*medium + 0.010*time + 0.010*like\n",
      "INFO : topic #24 (0.020): 0.029*jolie + 0.027*pitt + 0.019*couple + 0.015*child + 0.014*said + 0.014*source + 0.013*angelina + 0.011*year + 0.011*brad + 0.010*time\n",
      "INFO : topic #44 (0.020): 0.023*com + 0.022*quarter + 0.017*french + 0.017*home + 0.012*market + 0.012*statement + 0.011*house + 0.011*curbed + 0.011*mansion + 0.011*price\n",
      "INFO : topic #15 (0.020): 0.001*pitt + 0.001*jolie + 0.000*angelina + 0.000*brad + 0.000*cancer + 0.000*divorce + 0.000*couple + 0.000*year + 0.000*risk + 0.000*child\n",
      "INFO : topic diff=0.108253, rho=0.267261\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.650 per-word bound, 401.6 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 13, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #2 (0.020): 0.039*jolie + 0.032*heard + 0.012*pitt + 0.010*say + 0.010*think + 0.010*depp + 0.010*woman + 0.007*amber + 0.007*gun + 0.007*sander\n",
      "INFO : topic #33 (0.020): 0.000*cancer + 0.000*woman + 0.000*surgery + 0.000*breast + 0.000*jolie + 0.000*mastectomy + 0.000*risk + 0.000*clinic + 0.000*double + 0.000*angelina\n",
      "INFO : topic #47 (0.020): 0.049*pitt + 0.034*jolie + 0.020*movie + 0.012*brad + 0.011*gossip + 0.011*tabloid + 0.011*cop + 0.011*affleck + 0.009*child + 0.009*killed\n",
      "INFO : topic #23 (0.020): 0.047*jolie + 0.047*angelina + 0.038*pitt + 0.036*brad + 0.016*news + 0.014*george + 0.011*report + 0.011*said + 0.009*year + 0.009*couple\n",
      "INFO : topic #7 (0.020): 0.000*pitt + 0.000*jolie + 0.000*divorce + 0.000*couple + 0.000*angelina + 0.000*child + 0.000*brad + 0.000*said + 0.000*woman + 0.000*source\n",
      "INFO : topic diff=0.080655, rho=0.258199\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.646 per-word bound, 400.6 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 14, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #34 (0.020): 0.038*brad + 0.027*angelina + 0.016*aniston + 0.015*custody + 0.011*split + 0.010*source + 0.010*child + 0.009*tmz + 0.008*able + 0.008*dozen\n",
      "INFO : topic #4 (0.020): 0.032*jolie + 0.028*pitt + 0.021*angelina + 0.017*brad + 0.017*year + 0.015*child + 0.012*split + 0.012*news + 0.010*destructive + 0.010*self\n",
      "INFO : topic #33 (0.020): 0.000*cancer + 0.000*woman + 0.000*surgery + 0.000*breast + 0.000*jolie + 0.000*mastectomy + 0.000*risk + 0.000*clinic + 0.000*double + 0.000*angelina\n",
      "INFO : topic #26 (0.020): 0.038*pitt + 0.037*jolie + 0.034*divorce + 0.025*angelina + 0.024*brad + 0.013*film + 0.012*child + 0.012*say + 0.009*filed + 0.008*kid\n",
      "INFO : topic #15 (0.020): 0.000*pitt + 0.000*jolie + 0.000*angelina + 0.000*brad + 0.000*cancer + 0.000*divorce + 0.000*couple + 0.000*year + 0.000*risk + 0.000*child\n",
      "INFO : topic diff=0.060613, rho=0.250000\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.643 per-word bound, 399.9 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 15, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #19 (0.020): 0.045*brad + 0.043*angelina + 0.040*jolie + 0.037*pitt + 0.032*divorce + 0.016*report + 0.016*marion + 0.016*cotillard + 0.011*billy + 0.011*bob\n",
      "INFO : topic #29 (0.020): 0.023*clinic + 0.020*jolie + 0.018*procedure + 0.018*angelina + 0.018*mastectomy + 0.018*double + 0.017*preventative + 0.017*woman + 0.015*number + 0.015*prevention\n",
      "INFO : topic #9 (0.020): 0.000*pitt + 0.000*jolie + 0.000*couple + 0.000*divorce + 0.000*said + 0.000*angelina + 0.000*child + 0.000*year + 0.000*time + 0.000*brad\n",
      "INFO : topic #13 (0.020): 0.014*movie + 0.014*want + 0.014*news + 0.014*newsletter + 0.014*content + 0.014*giveaway + 0.014*signup + 0.014*latest + 0.014*exclusive + 0.014*trailer\n",
      "INFO : topic #35 (0.020): 0.025*grain + 0.015*brad + 0.015*source + 0.015*food + 0.015*ancient + 0.015*angie + 0.010*angelina + 0.010*said + 0.010*like + 0.010*year\n",
      "INFO : topic diff=0.045945, rho=0.242536\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.641 per-word bound, 399.3 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 16, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #20 (0.020): 0.000*frankenstein + 0.000*jolie + 0.000*bride + 0.000*rape + 0.000*universal + 0.000*pitt + 0.000*producer + 0.000*mummy + 0.000*lagoon + 0.000*film\n",
      "INFO : topic #46 (0.020): 0.029*peacekeeping + 0.029*woman + 0.022*right + 0.015*nation + 0.015*protection + 0.015*heart + 0.015*mom + 0.015*jolie + 0.015*actress + 0.007*conflict\n",
      "INFO : topic #13 (0.020): 0.014*movie + 0.014*want + 0.014*news + 0.014*newsletter + 0.014*content + 0.014*giveaway + 0.014*signup + 0.014*latest + 0.014*exclusive + 0.014*trailer\n",
      "INFO : topic #39 (0.020): 0.044*jolie + 0.038*film + 0.023*angelina + 0.017*marvel + 0.014*africa + 0.012*woman + 0.011*captain + 0.009*like + 0.008*wonder + 0.008*studio\n",
      "INFO : topic #0 (0.020): 0.061*cancer + 0.037*breast + 0.028*risk + 0.027*gene + 0.020*brca + 0.017*woman + 0.016*surgery + 0.016*jolie + 0.015*ovarian + 0.014*pitt\n",
      "INFO : topic diff=0.035125, rho=0.235702\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.640 per-word bound, 398.9 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 17, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #26 (0.020): 0.038*pitt + 0.037*jolie + 0.034*divorce + 0.025*angelina + 0.024*brad + 0.013*film + 0.012*child + 0.012*say + 0.009*filed + 0.008*kid\n",
      "INFO : topic #2 (0.020): 0.040*jolie + 0.032*heard + 0.012*pitt + 0.010*say + 0.010*think + 0.010*depp + 0.010*woman + 0.007*amber + 0.007*gun + 0.007*sander\n",
      "INFO : topic #3 (0.020): 0.000*world + 0.000*child + 0.000*eat + 0.000*starving + 0.000*sandwich + 0.000*report + 0.000*source + 0.000*jolie + 0.000*said + 0.000*angelina\n",
      "INFO : topic #18 (0.020): 0.000*pitt + 0.000*jolie + 0.000*divorce + 0.000*child + 0.000*couple + 0.000*brad + 0.000*angelina + 0.000*source + 0.000*said + 0.000*year\n",
      "INFO : topic #37 (0.020): 0.000*jolie + 0.000*pitt + 0.000*film + 0.000*woman + 0.000*angelina + 0.000*said + 0.000*heard + 0.000*khmer + 0.000*loung + 0.000*cancer\n",
      "INFO : topic diff=0.027080, rho=0.229416\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.639 per-word bound, 398.5 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 18, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #4 (0.020): 0.032*jolie + 0.028*pitt + 0.021*angelina + 0.017*brad + 0.017*year + 0.015*child + 0.012*split + 0.012*news + 0.010*destructive + 0.010*self\n",
      "INFO : topic #5 (0.020): 0.037*maleficent + 0.022*jolie + 0.019*film + 0.014*evil + 0.013*come + 0.012*story + 0.011*curse + 0.010*way + 0.009*fanning + 0.009*king\n",
      "INFO : topic #25 (0.020): 0.023*angelina + 0.022*woman + 0.019*peacekeeping + 0.017*jolie + 0.017*said + 0.016*summit + 0.011*abuse + 0.011*envoy + 0.011*special + 0.008*war\n",
      "INFO : topic #30 (0.020): 0.016*new + 0.016*people + 0.016*jolie + 0.016*child + 0.016*currently + 0.016*latest + 0.016*coordinate + 0.016*border + 0.008*stay + 0.008*speculation\n",
      "INFO : topic #7 (0.020): 0.000*pitt + 0.000*jolie + 0.000*divorce + 0.000*couple + 0.000*angelina + 0.000*child + 0.000*brad + 0.000*said + 0.000*woman + 0.000*source\n",
      "INFO : topic diff=0.021053, rho=0.223607\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.638 per-word bound, 398.3 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 19, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #23 (0.020): 0.047*jolie + 0.047*angelina + 0.038*pitt + 0.036*brad + 0.016*news + 0.014*george + 0.011*report + 0.011*said + 0.009*year + 0.009*couple\n",
      "INFO : topic #7 (0.020): 0.000*pitt + 0.000*jolie + 0.000*divorce + 0.000*couple + 0.000*angelina + 0.000*child + 0.000*brad + 0.000*said + 0.000*woman + 0.000*source\n",
      "INFO : topic #32 (0.020): 0.000*jolie + 0.000*angelina + 0.000*pitt + 0.000*brad + 0.000*divorce + 0.000*cotillard + 0.000*marion + 0.000*couple + 0.000*report + 0.000*star\n",
      "INFO : topic #48 (0.020): 0.040*jolie + 0.025*angelina + 0.020*brad + 0.020*new + 0.020*pitt + 0.015*normal + 0.010*movie + 0.010*maleficent + 0.010*red + 0.010*carpet\n",
      "INFO : topic #35 (0.020): 0.026*grain + 0.015*brad + 0.015*source + 0.015*food + 0.015*ancient + 0.015*angie + 0.010*angelina + 0.010*said + 0.010*like + 0.010*year\n",
      "INFO : topic diff=0.016502, rho=0.218218\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.637 per-word bound, 398.0 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 20, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #18 (0.020): 0.000*pitt + 0.000*jolie + 0.000*divorce + 0.000*child + 0.000*couple + 0.000*brad + 0.000*angelina + 0.000*source + 0.000*said + 0.000*year\n",
      "INFO : topic #31 (0.020): 0.029*jolie + 0.025*celebrity + 0.024*pitt + 0.014*brangelina + 0.013*couple + 0.013*said + 0.011*divorce + 0.011*medium + 0.010*time + 0.010*like\n",
      "INFO : topic #3 (0.020): 0.000*world + 0.000*child + 0.000*eat + 0.000*starving + 0.000*sandwich + 0.000*report + 0.000*source + 0.000*jolie + 0.000*said + 0.000*angelina\n",
      "INFO : topic #39 (0.020): 0.044*jolie + 0.038*film + 0.023*angelina + 0.017*marvel + 0.014*africa + 0.012*woman + 0.011*captain + 0.009*like + 0.008*wonder + 0.008*studio\n",
      "INFO : topic #45 (0.020): 0.000*woman + 0.000*jolie + 0.000*angelina + 0.000*peacekeeping + 0.000*right + 0.000*actress + 0.000*know + 0.000*cancer + 0.000*pitt + 0.000*mom\n",
      "INFO : topic diff=0.013038, rho=0.213201\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.636 per-word bound, 397.9 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 21, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #48 (0.020): 0.040*jolie + 0.025*angelina + 0.020*brad + 0.020*new + 0.020*pitt + 0.015*normal + 0.010*movie + 0.010*maleficent + 0.010*red + 0.010*carpet\n",
      "INFO : topic #34 (0.020): 0.038*brad + 0.026*angelina + 0.016*aniston + 0.015*custody + 0.011*split + 0.011*source + 0.010*child + 0.009*tmz + 0.008*able + 0.008*dozen\n",
      "INFO : topic #42 (0.020): 0.000*jolie + 0.000*pitt + 0.000*woman + 0.000*angelina + 0.000*cancer + 0.000*double + 0.000*clinic + 0.000*surgery + 0.000*preventative + 0.000*heard\n",
      "INFO : topic #14 (0.020): 0.033*rape + 0.020*sexual + 0.020*inevitable + 0.019*war + 0.013*shame + 0.013*violence + 0.013*feel + 0.013*global + 0.013*law + 0.013*rapist\n",
      "INFO : topic #9 (0.020): 0.000*pitt + 0.000*jolie + 0.000*couple + 0.000*divorce + 0.000*said + 0.000*angelina + 0.000*child + 0.000*year + 0.000*time + 0.000*brad\n",
      "INFO : topic diff=0.010382, rho=0.208514\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.636 per-word bound, 397.7 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 22, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #24 (0.020): 0.029*jolie + 0.027*pitt + 0.019*couple + 0.015*child + 0.015*said + 0.014*source + 0.013*angelina + 0.011*year + 0.010*brad + 0.010*time\n",
      "INFO : topic #30 (0.020): 0.016*new + 0.016*people + 0.016*jolie + 0.016*child + 0.016*currently + 0.016*latest + 0.016*coordinate + 0.016*border + 0.008*stay + 0.008*speculation\n",
      "INFO : topic #27 (0.020): 0.038*angelina + 0.026*source + 0.026*said + 0.026*jolie + 0.019*want + 0.013*dress + 0.013*friend + 0.013*actress + 0.013*report + 0.013*wear\n",
      "INFO : topic #9 (0.020): 0.000*pitt + 0.000*jolie + 0.000*couple + 0.000*divorce + 0.000*said + 0.000*angelina + 0.000*child + 0.000*year + 0.000*time + 0.000*brad\n",
      "INFO : topic #34 (0.020): 0.038*brad + 0.026*angelina + 0.016*aniston + 0.015*custody + 0.011*split + 0.011*source + 0.010*child + 0.009*tmz + 0.008*able + 0.008*dozen\n",
      "INFO : topic diff=0.008330, rho=0.204124\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.635 per-word bound, 397.6 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 23, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #2 (0.020): 0.040*jolie + 0.032*heard + 0.012*pitt + 0.010*say + 0.010*think + 0.010*depp + 0.010*woman + 0.007*amber + 0.007*gun + 0.007*sander\n",
      "INFO : topic #48 (0.020): 0.040*jolie + 0.025*angelina + 0.020*brad + 0.020*new + 0.020*pitt + 0.015*normal + 0.010*movie + 0.010*maleficent + 0.010*red + 0.010*carpet\n",
      "INFO : topic #15 (0.020): 0.000*pitt + 0.000*jolie + 0.000*angelina + 0.000*brad + 0.000*cancer + 0.000*divorce + 0.000*couple + 0.000*year + 0.000*risk + 0.000*child\n",
      "INFO : topic #41 (0.020): 0.000*woman + 0.000*pitt + 0.000*cancer + 0.000*jolie + 0.000*breast + 0.000*angelina + 0.000*brad + 0.000*think + 0.000*affleck + 0.000*tabloid\n",
      "INFO : topic #44 (0.020): 0.023*com + 0.022*quarter + 0.018*french + 0.017*home + 0.012*market + 0.012*statement + 0.011*house + 0.011*curbed + 0.011*mansion + 0.011*price\n",
      "INFO : topic diff=0.006732, rho=0.200000\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.635 per-word bound, 397.5 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 24, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #28 (0.020): 0.034*movie + 0.020*film + 0.020*angelina + 0.019*jolie + 0.014*day + 0.014*best + 0.014*star + 0.013*year + 0.007*news + 0.007*story\n",
      "INFO : topic #44 (0.020): 0.023*com + 0.022*quarter + 0.018*french + 0.017*home + 0.012*market + 0.012*statement + 0.011*house + 0.011*curbed + 0.011*mansion + 0.011*price\n",
      "INFO : topic #36 (0.020): 0.000*jolie + 0.000*film + 0.000*pitt + 0.000*angelina + 0.000*like + 0.000*woman + 0.000*couple + 0.000*mara + 0.000*pdt + 0.000*teigen\n",
      "INFO : topic #26 (0.020): 0.038*pitt + 0.037*jolie + 0.034*divorce + 0.025*angelina + 0.024*brad + 0.012*film + 0.012*child + 0.012*say + 0.009*filed + 0.008*kid\n",
      "INFO : topic #34 (0.020): 0.038*brad + 0.026*angelina + 0.016*aniston + 0.015*custody + 0.011*split + 0.011*source + 0.010*child + 0.009*tmz + 0.008*able + 0.008*dozen\n",
      "INFO : topic diff=0.005477, rho=0.196116\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.634 per-word bound, 397.4 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 25, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #33 (0.020): 0.000*cancer + 0.000*woman + 0.000*surgery + 0.000*breast + 0.000*jolie + 0.000*mastectomy + 0.000*risk + 0.000*clinic + 0.000*double + 0.000*angelina\n",
      "INFO : topic #34 (0.020): 0.038*brad + 0.026*angelina + 0.016*aniston + 0.015*custody + 0.011*split + 0.011*source + 0.010*child + 0.009*tmz + 0.008*able + 0.008*dozen\n",
      "INFO : topic #38 (0.020): 0.024*jolie + 0.024*teigen + 0.022*photo + 0.022*angelina + 0.019*mara + 0.014*pdt + 0.014*posted + 0.014*marateigen_ + 0.010*world + 0.010*smiling\n",
      "INFO : topic #26 (0.020): 0.038*pitt + 0.037*jolie + 0.034*divorce + 0.025*angelina + 0.024*brad + 0.012*film + 0.012*child + 0.012*say + 0.009*filed + 0.008*kid\n",
      "INFO : topic #24 (0.020): 0.029*jolie + 0.027*pitt + 0.019*couple + 0.015*child + 0.015*said + 0.014*source + 0.013*angelina + 0.011*year + 0.010*brad + 0.010*time\n",
      "INFO : topic diff=0.004485, rho=0.192450\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.634 per-word bound, 397.3 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 26, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #19 (0.020): 0.045*brad + 0.043*angelina + 0.041*jolie + 0.038*pitt + 0.032*divorce + 0.016*report + 0.016*marion + 0.016*cotillard + 0.010*billy + 0.010*bob\n",
      "INFO : topic #34 (0.020): 0.038*brad + 0.026*angelina + 0.016*aniston + 0.015*custody + 0.011*split + 0.011*source + 0.010*child + 0.009*tmz + 0.008*able + 0.008*dozen\n",
      "INFO : topic #48 (0.020): 0.040*jolie + 0.025*angelina + 0.020*brad + 0.020*new + 0.020*pitt + 0.015*normal + 0.010*movie + 0.010*maleficent + 0.010*red + 0.010*carpet\n",
      "INFO : topic #46 (0.020): 0.029*peacekeeping + 0.029*woman + 0.022*right + 0.015*nation + 0.015*protection + 0.015*heart + 0.015*mom + 0.015*jolie + 0.015*actress + 0.007*conflict\n",
      "INFO : topic #6 (0.020): 0.030*woman + 0.019*breast + 0.018*cancer + 0.017*angelina + 0.016*jolie + 0.010*awareness + 0.009*treatment + 0.009*raise + 0.008*test + 0.007*mastectomy\n",
      "INFO : topic diff=0.003695, rho=0.188982\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.634 per-word bound, 397.2 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 27, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #44 (0.020): 0.023*com + 0.022*quarter + 0.018*french + 0.017*home + 0.012*statement + 0.012*market + 0.011*house + 0.011*curbed + 0.011*mansion + 0.011*price\n",
      "INFO : topic #37 (0.020): 0.000*jolie + 0.000*pitt + 0.000*film + 0.000*woman + 0.000*angelina + 0.000*said + 0.000*heard + 0.000*khmer + 0.000*loung + 0.000*cancer\n",
      "INFO : topic #29 (0.020): 0.022*clinic + 0.020*jolie + 0.018*procedure + 0.018*angelina + 0.018*mastectomy + 0.018*double + 0.017*preventative + 0.017*woman + 0.015*number + 0.015*prevention\n",
      "INFO : topic #49 (0.020): 0.051*pitt + 0.030*divorce + 0.027*jolie + 0.020*couple + 0.020*year + 0.019*brad + 0.015*marriage + 0.015*bohringer + 0.015*radar + 0.015*drinking\n",
      "INFO : topic #30 (0.020): 0.016*new + 0.016*people + 0.016*child + 0.016*jolie + 0.016*currently + 0.016*latest + 0.016*coordinate + 0.016*border + 0.008*stay + 0.008*speculation\n",
      "INFO : topic diff=0.003062, rho=0.185695\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.634 per-word bound, 397.2 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 28, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #19 (0.020): 0.045*brad + 0.043*angelina + 0.041*jolie + 0.038*pitt + 0.032*divorce + 0.016*report + 0.016*marion + 0.016*cotillard + 0.010*billy + 0.010*bob\n",
      "INFO : topic #6 (0.020): 0.030*woman + 0.019*breast + 0.018*cancer + 0.017*angelina + 0.016*jolie + 0.010*awareness + 0.009*treatment + 0.009*raise + 0.008*test + 0.007*mastectomy\n",
      "INFO : topic #4 (0.020): 0.032*jolie + 0.028*pitt + 0.021*angelina + 0.017*brad + 0.017*year + 0.015*child + 0.013*split + 0.012*news + 0.010*know + 0.010*destructive\n",
      "INFO : topic #11 (0.020): 0.000*pitt + 0.000*jolie + 0.000*film + 0.000*read + 0.000*angelina + 0.000*said + 0.000*couple + 0.000*brad + 0.000*child + 0.000*father\n",
      "INFO : topic #23 (0.020): 0.047*jolie + 0.047*angelina + 0.038*pitt + 0.036*brad + 0.016*news + 0.014*george + 0.011*report + 0.011*said + 0.009*year + 0.009*couple\n",
      "INFO : topic diff=0.002552, rho=0.182574\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.633 per-word bound, 397.1 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 29, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #38 (0.020): 0.024*jolie + 0.024*teigen + 0.022*photo + 0.022*angelina + 0.019*mara + 0.014*pdt + 0.014*posted + 0.014*marateigen_ + 0.010*world + 0.010*smiling\n",
      "INFO : topic #19 (0.020): 0.045*brad + 0.043*angelina + 0.041*jolie + 0.038*pitt + 0.032*divorce + 0.016*report + 0.016*marion + 0.016*cotillard + 0.010*billy + 0.010*bob\n",
      "INFO : topic #36 (0.020): 0.000*jolie + 0.000*film + 0.000*pitt + 0.000*angelina + 0.000*like + 0.000*woman + 0.000*couple + 0.000*mara + 0.000*pdt + 0.000*teigen\n",
      "INFO : topic #10 (0.020): 0.000*pitt + 0.000*jolie + 0.000*brad + 0.000*source + 0.000*couple + 0.000*said + 0.000*affleck + 0.000*angelina + 0.000*cop + 0.000*child\n",
      "INFO : topic #40 (0.020): 0.031*jolie + 0.021*acting + 0.019*career + 0.018*year + 0.018*angelina + 0.016*director + 0.016*certainly + 0.012*drama + 0.010*actress + 0.009*debut\n",
      "INFO : topic diff=0.002139, rho=0.179605\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.633 per-word bound, 397.1 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 30, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #44 (0.020): 0.023*com + 0.022*quarter + 0.018*french + 0.017*home + 0.012*statement + 0.012*market + 0.011*house + 0.011*curbed + 0.011*mansion + 0.011*price\n",
      "INFO : topic #12 (0.020): 0.026*detail + 0.013*project + 0.013*tell + 0.013*today + 0.013*finally + 0.013*focus + 0.013*won + 0.013*far + 0.013*fact + 0.013*future\n",
      "INFO : topic #21 (0.020): 0.000*pitt + 0.000*jolie + 0.000*woman + 0.000*angelina + 0.000*brad + 0.000*year + 0.000*breast + 0.000*divorce + 0.000*cancer + 0.000*film\n",
      "INFO : topic #35 (0.020): 0.026*grain + 0.015*brad + 0.015*source + 0.015*food + 0.015*ancient + 0.015*angie + 0.010*angelina + 0.010*said + 0.010*like + 0.010*year\n",
      "INFO : topic #39 (0.020): 0.044*jolie + 0.038*film + 0.023*angelina + 0.017*marvel + 0.014*africa + 0.012*woman + 0.011*captain + 0.009*like + 0.008*wonder + 0.008*studio\n",
      "INFO : topic diff=0.001802, rho=0.176777\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.633 per-word bound, 397.0 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 31, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #38 (0.020): 0.024*jolie + 0.024*teigen + 0.022*photo + 0.022*angelina + 0.019*mara + 0.014*pdt + 0.014*posted + 0.014*marateigen_ + 0.010*world + 0.010*smiling\n",
      "INFO : topic #45 (0.020): 0.000*woman + 0.000*jolie + 0.000*angelina + 0.000*peacekeeping + 0.000*right + 0.000*actress + 0.000*know + 0.000*cancer + 0.000*pitt + 0.000*mom\n",
      "INFO : topic #29 (0.020): 0.022*clinic + 0.020*jolie + 0.018*procedure + 0.018*angelina + 0.018*mastectomy + 0.018*double + 0.017*preventative + 0.017*woman + 0.015*number + 0.015*prevention\n",
      "INFO : topic #25 (0.020): 0.023*angelina + 0.022*woman + 0.019*peacekeeping + 0.017*jolie + 0.017*said + 0.016*summit + 0.011*abuse + 0.011*envoy + 0.011*special + 0.008*war\n",
      "INFO : topic #23 (0.020): 0.047*jolie + 0.047*angelina + 0.038*pitt + 0.036*brad + 0.016*news + 0.014*george + 0.011*report + 0.011*said + 0.009*year + 0.009*couple\n",
      "INFO : topic diff=0.001526, rho=0.174078\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.633 per-word bound, 397.0 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 32, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #5 (0.020): 0.037*maleficent + 0.022*jolie + 0.019*film + 0.014*evil + 0.013*come + 0.012*story + 0.011*curse + 0.010*way + 0.009*fanning + 0.009*king\n",
      "INFO : topic #33 (0.020): 0.000*cancer + 0.000*woman + 0.000*surgery + 0.000*breast + 0.000*jolie + 0.000*mastectomy + 0.000*risk + 0.000*clinic + 0.000*double + 0.000*angelina\n",
      "INFO : topic #18 (0.020): 0.000*pitt + 0.000*jolie + 0.000*divorce + 0.000*child + 0.000*couple + 0.000*brad + 0.000*angelina + 0.000*source + 0.000*said + 0.000*year\n",
      "INFO : topic #13 (0.020): 0.014*movie + 0.014*want + 0.014*news + 0.014*newsletter + 0.014*content + 0.014*giveaway + 0.014*signup + 0.014*latest + 0.014*exclusive + 0.014*trailer\n",
      "INFO : topic #43 (0.020): 0.000*pitt + 0.000*jolie + 0.000*couple + 0.000*divorce + 0.000*child + 0.000*said + 0.000*angelina + 0.000*time + 0.000*year + 0.000*cancer\n",
      "INFO : topic diff=0.001299, rho=0.171499\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.633 per-word bound, 397.0 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 33, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #0 (0.020): 0.058*cancer + 0.036*breast + 0.028*risk + 0.028*gene + 0.020*brca + 0.016*woman + 0.016*surgery + 0.016*ovarian + 0.016*jolie + 0.014*pitt\n",
      "INFO : topic #13 (0.020): 0.014*movie + 0.014*want + 0.014*news + 0.014*newsletter + 0.014*content + 0.014*giveaway + 0.014*signup + 0.014*latest + 0.014*exclusive + 0.014*trailer\n",
      "INFO : topic #43 (0.020): 0.000*pitt + 0.000*jolie + 0.000*couple + 0.000*divorce + 0.000*child + 0.000*said + 0.000*angelina + 0.000*time + 0.000*year + 0.000*cancer\n",
      "INFO : topic #48 (0.020): 0.040*jolie + 0.025*angelina + 0.020*brad + 0.020*new + 0.020*pitt + 0.015*normal + 0.010*movie + 0.010*maleficent + 0.010*red + 0.010*carpet\n",
      "INFO : topic #7 (0.020): 0.000*pitt + 0.000*jolie + 0.000*divorce + 0.000*couple + 0.000*angelina + 0.000*child + 0.000*brad + 0.000*said + 0.000*woman + 0.000*source\n",
      "INFO : topic diff=0.001111, rho=0.169031\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.633 per-word bound, 396.9 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 34, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #39 (0.020): 0.044*jolie + 0.038*film + 0.023*angelina + 0.017*marvel + 0.014*africa + 0.012*woman + 0.011*captain + 0.009*like + 0.008*wonder + 0.008*studio\n",
      "INFO : topic #35 (0.020): 0.026*grain + 0.015*brad + 0.015*source + 0.015*food + 0.015*ancient + 0.015*angie + 0.010*angelina + 0.010*said + 0.010*like + 0.010*year\n",
      "INFO : topic #14 (0.020): 0.033*rape + 0.020*sexual + 0.020*inevitable + 0.019*war + 0.013*shame + 0.013*violence + 0.013*feel + 0.013*global + 0.013*law + 0.013*rapist\n",
      "INFO : topic #43 (0.020): 0.000*pitt + 0.000*jolie + 0.000*couple + 0.000*divorce + 0.000*child + 0.000*said + 0.000*angelina + 0.000*time + 0.000*year + 0.000*cancer\n",
      "INFO : topic #37 (0.020): 0.000*jolie + 0.000*pitt + 0.000*film + 0.000*woman + 0.000*angelina + 0.000*said + 0.000*heard + 0.000*khmer + 0.000*loung + 0.000*cancer\n",
      "INFO : topic diff=0.000956, rho=0.166667\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.633 per-word bound, 396.9 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 35, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #16 (0.020): 0.033*frankenstein + 0.019*bride + 0.014*jolie + 0.014*mummy + 0.014*universal + 0.010*working + 0.010*character + 0.010*film + 0.010*time + 0.010*pitt\n",
      "INFO : topic #23 (0.020): 0.047*jolie + 0.047*angelina + 0.038*pitt + 0.036*brad + 0.016*news + 0.014*george + 0.011*report + 0.011*said + 0.009*year + 0.009*couple\n",
      "INFO : topic #30 (0.020): 0.016*new + 0.016*people + 0.016*child + 0.016*jolie + 0.016*currently + 0.016*latest + 0.016*coordinate + 0.016*border + 0.008*stay + 0.008*speculation\n",
      "INFO : topic #17 (0.020): 0.000*jolie + 0.000*pitt + 0.000*angelina + 0.000*divorce + 0.000*heard + 0.000*film + 0.000*brad + 0.000*say + 0.000*child + 0.000*couple\n",
      "INFO : topic #42 (0.020): 0.000*jolie + 0.000*pitt + 0.000*woman + 0.000*angelina + 0.000*cancer + 0.000*double + 0.000*clinic + 0.000*surgery + 0.000*preventative + 0.000*heard\n",
      "INFO : topic diff=0.000826, rho=0.164399\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.633 per-word bound, 396.9 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 36, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #21 (0.020): 0.000*pitt + 0.000*jolie + 0.000*woman + 0.000*angelina + 0.000*brad + 0.000*year + 0.000*breast + 0.000*divorce + 0.000*cancer + 0.000*film\n",
      "INFO : topic #14 (0.020): 0.033*rape + 0.020*sexual + 0.020*inevitable + 0.019*war + 0.013*shame + 0.013*violence + 0.013*feel + 0.013*global + 0.013*law + 0.013*rapist\n",
      "INFO : topic #29 (0.020): 0.022*clinic + 0.020*jolie + 0.018*procedure + 0.018*angelina + 0.018*mastectomy + 0.018*double + 0.017*preventative + 0.017*woman + 0.015*number + 0.015*prevention\n",
      "INFO : topic #3 (0.020): 0.000*world + 0.000*child + 0.000*eat + 0.000*starving + 0.000*sandwich + 0.000*report + 0.000*source + 0.000*jolie + 0.000*said + 0.000*angelina\n",
      "INFO : topic #1 (0.020): 0.025*conscious + 0.022*open + 0.016*role + 0.012*mail + 0.012*day + 0.012*fair + 0.012*globe + 0.011*true + 0.011*enter + 0.011*complete\n",
      "INFO : topic diff=0.000718, rho=0.162221\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.632 per-word bound, 396.9 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 37, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #16 (0.020): 0.033*frankenstein + 0.019*bride + 0.014*jolie + 0.014*mummy + 0.014*universal + 0.010*working + 0.010*character + 0.010*film + 0.010*time + 0.010*pitt\n",
      "INFO : topic #28 (0.020): 0.034*movie + 0.020*film + 0.020*angelina + 0.019*jolie + 0.014*day + 0.014*best + 0.014*star + 0.013*year + 0.007*news + 0.007*story\n",
      "INFO : topic #1 (0.020): 0.025*conscious + 0.022*open + 0.016*role + 0.012*mail + 0.012*day + 0.012*fair + 0.012*globe + 0.011*true + 0.011*enter + 0.011*complete\n",
      "INFO : topic #15 (0.020): 0.000*pitt + 0.000*jolie + 0.000*angelina + 0.000*brad + 0.000*cancer + 0.000*divorce + 0.000*couple + 0.000*year + 0.000*risk + 0.000*child\n",
      "INFO : topic #6 (0.020): 0.030*woman + 0.019*breast + 0.019*cancer + 0.016*angelina + 0.016*jolie + 0.010*awareness + 0.009*treatment + 0.009*raise + 0.008*test + 0.007*mastectomy\n",
      "INFO : topic diff=0.000627, rho=0.160128\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.632 per-word bound, 396.8 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 38, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #27 (0.020): 0.038*angelina + 0.026*source + 0.026*said + 0.026*jolie + 0.019*want + 0.013*dress + 0.013*friend + 0.013*actress + 0.013*report + 0.013*wear\n",
      "INFO : topic #32 (0.020): 0.000*jolie + 0.000*angelina + 0.000*pitt + 0.000*brad + 0.000*divorce + 0.000*cotillard + 0.000*marion + 0.000*couple + 0.000*report + 0.000*star\n",
      "INFO : topic #11 (0.020): 0.000*pitt + 0.000*jolie + 0.000*film + 0.000*read + 0.000*angelina + 0.000*said + 0.000*couple + 0.000*brad + 0.000*child + 0.000*father\n",
      "INFO : topic #37 (0.020): 0.000*jolie + 0.000*pitt + 0.000*film + 0.000*woman + 0.000*angelina + 0.000*said + 0.000*heard + 0.000*khmer + 0.000*loung + 0.000*cancer\n",
      "INFO : topic #16 (0.020): 0.033*frankenstein + 0.019*bride + 0.014*jolie + 0.014*mummy + 0.014*universal + 0.010*working + 0.010*character + 0.010*film + 0.010*time + 0.010*pitt\n",
      "INFO : topic diff=0.000551, rho=0.158114\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -8.632 per-word bound, 396.8 perplexity estimate based on a held-out corpus of 54 documents with 10317 words\n",
      "INFO : PROGRESS: pass 39, at document #54/54\n",
      "DEBUG : performing inference on a chunk of 54 documents\n",
      "DEBUG : 54/54 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "INFO : topic #14 (0.020): 0.033*rape + 0.020*sexual + 0.020*inevitable + 0.019*war + 0.013*shame + 0.013*violence + 0.013*feel + 0.013*global + 0.013*law + 0.013*rapist\n",
      "INFO : topic #16 (0.020): 0.033*frankenstein + 0.019*bride + 0.014*jolie + 0.014*mummy + 0.014*universal + 0.010*working + 0.010*character + 0.010*film + 0.010*time + 0.010*pitt\n",
      "INFO : topic #18 (0.020): 0.000*pitt + 0.000*jolie + 0.000*divorce + 0.000*child + 0.000*couple + 0.000*brad + 0.000*angelina + 0.000*source + 0.000*said + 0.000*year\n",
      "INFO : topic #1 (0.020): 0.025*conscious + 0.022*open + 0.016*role + 0.012*mail + 0.012*day + 0.012*fair + 0.012*globe + 0.011*true + 0.011*enter + 0.011*complete\n",
      "INFO : topic #30 (0.020): 0.016*new + 0.016*people + 0.016*child + 0.016*jolie + 0.016*currently + 0.016*latest + 0.016*coordinate + 0.016*border + 0.008*stay + 0.008*speculation\n",
      "INFO : topic diff=0.000486, rho=0.156174\n",
      "INFO : saving LdaState object under ./MODELS/jolie_lemmatized_tfidf_lda.model.state, separately None\n",
      "INFO : saving LdaModel object under ./MODELS/jolie_lemmatized_tfidf_lda.model, separately None\n",
      "INFO : not storing attribute state\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : saving LdaState object under ./MODELS/jolie_lemmatized_lda.model.state, separately None\n",
      "INFO : saving LdaModel object under ./MODELS/jolie_lemmatized_lda.model, separately None\n",
      "INFO : not storing attribute state\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : loading LdaModel object from ./MODELS/jolie_lemmatized_tfidf_lda.model\n",
      "INFO : loading id2word recursively from ./MODELS/jolie_lemmatized_tfidf_lda.model.id2word.* with mmap=None\n",
      "INFO : setting ignored attribute state to None\n",
      "INFO : setting ignored attribute dispatcher to None\n",
      "INFO : loading LdaModel object from ./MODELS/jolie_lemmatized_tfidf_lda.model.state\n",
      "INFO : loaded corpus index from ./CORPUSES/jolie_lemmatized.mm.index\n",
      "INFO : initializing corpus reader from ./CORPUSES/jolie_lemmatized.mm\n",
      "INFO : accepted corpus with 54 documents, 3085 features, 6621 non-zero entries\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1st Most Discussed Topic (MDT)</th>\n",
       "      <td>0.015*divorce + 0.014*pitt + 0.013*brad + 0.010*child + 0.010*said + 0.010*couple + 0.008*split + 0.008*year + 0.007*family + 0.007*filed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2nd MDT</th>\n",
       "      <td>0.009*source + 0.006*report + 0.006*cotillard + 0.006*marion + 0.006*like + 0.006*say + 0.006*want + 0.005*claim + 0.005*destructive + 0.005*set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd MDT</th>\n",
       "      <td>0.012*film + 0.009*woman + 0.006*star + 0.005*oscar + 0.005*hollywood + 0.005*movie + 0.005*acting + 0.004*director + 0.004*certainly + 0.004*think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4th MDT</th>\n",
       "      <td>0.011*inbox + 0.007*latest + 0.007*loung + 0.006*ung + 0.005*giveaway + 0.005*content + 0.005*signup + 0.005*news + 0.005*trailer + 0.005*khmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5th MDT</th>\n",
       "      <td>0.007*heard + 0.007*news + 0.006*kid + 0.005*matter + 0.005*tuesday + 0.005*statement + 0.005*press + 0.004*later + 0.004*list + 0.004*privacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6th MDT</th>\n",
       "      <td>0.007*father + 0.007*killed + 0.007*cambodia + 0.005*son + 0.005*movie + 0.005*scene + 0.005*people + 0.005*tell + 0.005*work + 0.004*normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7th MDT</th>\n",
       "      <td>0.019*cancer + 0.015*breast + 0.008*gene + 0.007*mastectomy + 0.006*risk + 0.005*surgery + 0.005*double + 0.005*brca + 0.004*mutation + 0.004*preventive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8th MDT</th>\n",
       "      <td>0.005*read + 0.004*exposed + 0.004*sandwich + 0.004*starving + 0.004*eat + 0.003*love + 0.003*course + 0.003*surprising + 0.003*hour + 0.003*shocking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9th MDT</th>\n",
       "      <td>0.007*marvel + 0.006*gazing + 0.006*horoscope + 0.005*finally + 0.005*captain + 0.004*future + 0.004*finished + 0.004*stop + 0.004*far + 0.004*superhero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10th MDT</th>\n",
       "      <td>0.006*rape + 0.004*hand + 0.004*inevitable + 0.003*month + 0.003*march + 0.003*london + 0.003*war + 0.003*violence + 0.003*shame + 0.003*survivor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                       0\n",
       "1st Most Discussed Topic (MDT)                 0.015*divorce + 0.014*pitt + 0.013*brad + 0.010*child + 0.010*said + 0.010*couple + 0.008*split + 0.008*year + 0.007*family + 0.007*filed\n",
       "2nd MDT                                 0.009*source + 0.006*report + 0.006*cotillard + 0.006*marion + 0.006*like + 0.006*say + 0.006*want + 0.005*claim + 0.005*destructive + 0.005*set\n",
       "3rd MDT                              0.012*film + 0.009*woman + 0.006*star + 0.005*oscar + 0.005*hollywood + 0.005*movie + 0.005*acting + 0.004*director + 0.004*certainly + 0.004*think\n",
       "4th MDT                                  0.011*inbox + 0.007*latest + 0.007*loung + 0.006*ung + 0.005*giveaway + 0.005*content + 0.005*signup + 0.005*news + 0.005*trailer + 0.005*khmer\n",
       "5th MDT                                   0.007*heard + 0.007*news + 0.006*kid + 0.005*matter + 0.005*tuesday + 0.005*statement + 0.005*press + 0.004*later + 0.004*list + 0.004*privacy\n",
       "6th MDT                                     0.007*father + 0.007*killed + 0.007*cambodia + 0.005*son + 0.005*movie + 0.005*scene + 0.005*people + 0.005*tell + 0.005*work + 0.004*normal\n",
       "7th MDT                         0.019*cancer + 0.015*breast + 0.008*gene + 0.007*mastectomy + 0.006*risk + 0.005*surgery + 0.005*double + 0.005*brca + 0.004*mutation + 0.004*preventive\n",
       "8th MDT                            0.005*read + 0.004*exposed + 0.004*sandwich + 0.004*starving + 0.004*eat + 0.003*love + 0.003*course + 0.003*surprising + 0.003*hour + 0.003*shocking\n",
       "9th MDT                         0.007*marvel + 0.006*gazing + 0.006*horoscope + 0.005*finally + 0.005*captain + 0.004*future + 0.004*finished + 0.004*stop + 0.004*far + 0.004*superhero\n",
       "10th MDT                               0.006*rape + 0.004*hand + 0.004*inevitable + 0.003*month + 0.003*march + 0.003*london + 0.003*war + 0.003*violence + 0.003*shame + 0.003*survivor"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create corpus, checking for duplicate articles\n",
    "%run create_corpuses.py \"DATA/jolie\" jolie  --nodup\n",
    "\n",
    "# Create LDA_TFIDF stacked model for our corpus\n",
    "%run create_models.py \"jolie\"  50 25  # 50 topics and 25 passes\n",
    "\n",
    "# Toggle off processing output since its large...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loading LdaModel object from ./MODELS/jolie_lemmatized_tfidf_lda.model\n",
      "INFO : loading id2word recursively from ./MODELS/jolie_lemmatized_tfidf_lda.model.id2word.* with mmap=None\n",
      "INFO : setting ignored attribute state to None\n",
      "INFO : setting ignored attribute dispatcher to None\n",
      "INFO : loading LdaModel object from ./MODELS/jolie_lemmatized_tfidf_lda.model.state\n",
      "INFO : loaded corpus index from ./CORPUSES/jolie_lemmatized.mm.index\n",
      "INFO : initializing corpus reader from ./CORPUSES/jolie_lemmatized.mm\n",
      "INFO : accepted corpus with 54 documents, 3085 features, 6621 non-zero entries\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1st Most Discussed Topic (MDT)</th>\n",
       "      <td>0.015*divorce + 0.014*pitt + 0.013*brad + 0.010*child + 0.010*said + 0.010*couple + 0.008*split + 0.008*year + 0.007*family + 0.007*filed + 0.006*old + 0.006*new + 0.006*actress + 0.006*actor + 0.006*know + 0.006*custody + 0.006*time + 0.006*way + 0.006*celebrity + 0.005*day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2nd MDT</th>\n",
       "      <td>0.009*source + 0.006*report + 0.006*cotillard + 0.006*marion + 0.006*like + 0.006*say + 0.006*want + 0.005*claim + 0.005*destructive + 0.005*set + 0.005*photo + 0.005*self + 0.004*claimed + 0.004*rumor + 0.004*comment + 0.004*friend + 0.004*husband + 0.004*going + 0.004*maddox + 0.004*advertisement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd MDT</th>\n",
       "      <td>0.012*film + 0.009*woman + 0.006*star + 0.005*oscar + 0.005*hollywood + 0.005*movie + 0.005*acting + 0.004*director + 0.004*certainly + 0.004*think + 0.004*right + 0.004*career + 0.004*working + 0.004*look + 0.004*read + 0.004*direct + 0.004*debut + 0.004*second + 0.004*having + 0.003*home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4th MDT</th>\n",
       "      <td>0.011*inbox + 0.007*latest + 0.007*loung + 0.006*ung + 0.005*giveaway + 0.005*content + 0.005*signup + 0.005*news + 0.005*trailer + 0.005*khmer + 0.005*newsletter + 0.005*book + 0.004*coordinate + 0.004*rouge + 0.004*exclusive + 0.004*cambodian + 0.004*currently + 0.004*day + 0.003*story + 0.003*born</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5th MDT</th>\n",
       "      <td>0.007*heard + 0.007*news + 0.006*kid + 0.005*matter + 0.005*tuesday + 0.005*statement + 0.005*press + 0.004*later + 0.004*list + 0.004*privacy + 0.004*confirmed + 0.004*ask + 0.004*challenging + 0.004*space + 0.004*kindly + 0.004*saddened + 0.004*deserve + 0.003*break + 0.003*manager + 0.003*geyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6th MDT</th>\n",
       "      <td>0.007*father + 0.007*killed + 0.007*cambodia + 0.005*son + 0.005*movie + 0.005*scene + 0.005*people + 0.005*tell + 0.005*work + 0.004*normal + 0.004*little + 0.004*directed + 0.004*face + 0.004*red + 0.004*carpet + 0.004*issue + 0.003*oldest + 0.003*start + 0.003*onscreen + 0.003*went</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7th MDT</th>\n",
       "      <td>0.019*cancer + 0.015*breast + 0.008*gene + 0.007*mastectomy + 0.006*risk + 0.005*surgery + 0.005*double + 0.005*brca + 0.004*mutation + 0.004*preventive + 0.003*ovarian + 0.003*developing + 0.003*increase + 0.003*ovary + 0.003*tube + 0.003*underwent + 0.003*chance + 0.003*fallopian + 0.002*removing + 0.002*hormone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8th MDT</th>\n",
       "      <td>0.005*read + 0.004*exposed + 0.004*sandwich + 0.004*starving + 0.004*eat + 0.003*love + 0.003*course + 0.003*surprising + 0.003*hour + 0.003*shocking + 0.002*sexy + 0.002*pretty + 0.002*stealing + 0.002*kim + 0.002*wenn + 0.002*make + 0.002*knot + 0.002*hot + 0.002*lost + 0.002*tied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9th MDT</th>\n",
       "      <td>0.007*marvel + 0.006*gazing + 0.006*horoscope + 0.005*finally + 0.005*captain + 0.004*future + 0.004*finished + 0.004*stop + 0.004*far + 0.004*superhero + 0.004*detail + 0.004*sign + 0.003*focus + 0.003*won + 0.003*project + 0.003*fact + 0.003*africa + 0.003*studio + 0.003*wonder + 0.003*today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10th MDT</th>\n",
       "      <td>0.006*rape + 0.004*hand + 0.004*inevitable + 0.003*month + 0.003*march + 0.003*london + 0.003*war + 0.003*violence + 0.003*shame + 0.003*survivor + 0.003*rapist + 0.003*arriving + 0.003*sexual + 0.003*global + 0.003*terminal + 0.003*lax + 0.003*exited + 0.003*smoke + 0.003*visit + 0.003*signature</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                          0\n",
       "1st Most Discussed Topic (MDT)                                          0.015*divorce + 0.014*pitt + 0.013*brad + 0.010*child + 0.010*said + 0.010*couple + 0.008*split + 0.008*year + 0.007*family + 0.007*filed + 0.006*old + 0.006*new + 0.006*actress + 0.006*actor + 0.006*know + 0.006*custody + 0.006*time + 0.006*way + 0.006*celebrity + 0.005*day\n",
       "2nd MDT                                         0.009*source + 0.006*report + 0.006*cotillard + 0.006*marion + 0.006*like + 0.006*say + 0.006*want + 0.005*claim + 0.005*destructive + 0.005*set + 0.005*photo + 0.005*self + 0.004*claimed + 0.004*rumor + 0.004*comment + 0.004*friend + 0.004*husband + 0.004*going + 0.004*maddox + 0.004*advertisement\n",
       "3rd MDT                                                  0.012*film + 0.009*woman + 0.006*star + 0.005*oscar + 0.005*hollywood + 0.005*movie + 0.005*acting + 0.004*director + 0.004*certainly + 0.004*think + 0.004*right + 0.004*career + 0.004*working + 0.004*look + 0.004*read + 0.004*direct + 0.004*debut + 0.004*second + 0.004*having + 0.003*home\n",
       "4th MDT                                       0.011*inbox + 0.007*latest + 0.007*loung + 0.006*ung + 0.005*giveaway + 0.005*content + 0.005*signup + 0.005*news + 0.005*trailer + 0.005*khmer + 0.005*newsletter + 0.005*book + 0.004*coordinate + 0.004*rouge + 0.004*exclusive + 0.004*cambodian + 0.004*currently + 0.004*day + 0.003*story + 0.003*born\n",
       "5th MDT                                          0.007*heard + 0.007*news + 0.006*kid + 0.005*matter + 0.005*tuesday + 0.005*statement + 0.005*press + 0.004*later + 0.004*list + 0.004*privacy + 0.004*confirmed + 0.004*ask + 0.004*challenging + 0.004*space + 0.004*kindly + 0.004*saddened + 0.004*deserve + 0.003*break + 0.003*manager + 0.003*geyer\n",
       "6th MDT                                                       0.007*father + 0.007*killed + 0.007*cambodia + 0.005*son + 0.005*movie + 0.005*scene + 0.005*people + 0.005*tell + 0.005*work + 0.004*normal + 0.004*little + 0.004*directed + 0.004*face + 0.004*red + 0.004*carpet + 0.004*issue + 0.003*oldest + 0.003*start + 0.003*onscreen + 0.003*went\n",
       "7th MDT                         0.019*cancer + 0.015*breast + 0.008*gene + 0.007*mastectomy + 0.006*risk + 0.005*surgery + 0.005*double + 0.005*brca + 0.004*mutation + 0.004*preventive + 0.003*ovarian + 0.003*developing + 0.003*increase + 0.003*ovary + 0.003*tube + 0.003*underwent + 0.003*chance + 0.003*fallopian + 0.002*removing + 0.002*hormone\n",
       "8th MDT                                                         0.005*read + 0.004*exposed + 0.004*sandwich + 0.004*starving + 0.004*eat + 0.003*love + 0.003*course + 0.003*surprising + 0.003*hour + 0.003*shocking + 0.002*sexy + 0.002*pretty + 0.002*stealing + 0.002*kim + 0.002*wenn + 0.002*make + 0.002*knot + 0.002*hot + 0.002*lost + 0.002*tied\n",
       "9th MDT                                              0.007*marvel + 0.006*gazing + 0.006*horoscope + 0.005*finally + 0.005*captain + 0.004*future + 0.004*finished + 0.004*stop + 0.004*far + 0.004*superhero + 0.004*detail + 0.004*sign + 0.003*focus + 0.003*won + 0.003*project + 0.003*fact + 0.003*africa + 0.003*studio + 0.003*wonder + 0.003*today\n",
       "10th MDT                                          0.006*rape + 0.004*hand + 0.004*inevitable + 0.003*month + 0.003*march + 0.003*london + 0.003*war + 0.003*violence + 0.003*shame + 0.003*survivor + 0.003*rapist + 0.003*arriving + 0.003*sexual + 0.003*global + 0.003*terminal + 0.003*lax + 0.003*exited + 0.003*smoke + 0.003*visit + 0.003*signature"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the jolie LDA model and corpus (which are saved to ./MODELS and ./CORPUSES\n",
    "# by running the create_models and create_corpuses scripts)\n",
    "panda_topics(\"./MODELS/jolie_lemmatized_tfidf_lda.model\" , \"./CORPUSES/jolie_lemmatized.mm\", 20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
